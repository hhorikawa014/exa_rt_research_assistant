{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1p0SJWzAPNXm_AVYCbecIVoESX7xXSh-P","timestamp":1742509976306}],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyPOZjg1BVx72pG63JrB/odg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mc3Cf334sskm","executionInfo":{"status":"ok","timestamp":1742788955812,"user_tz":420,"elapsed":18014,"user":{"displayName":"H H","userId":"04764467037930588626"}},"outputId":"4093fec0-f173-4c7d-80b9-06e126798d6d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_lcR-FteLQi","executionInfo":{"status":"ok","timestamp":1742788960136,"user_tz":420,"elapsed":4321,"user":{"displayName":"H H","userId":"04764467037930588626"}},"outputId":"4d80df39-935c-4df4-8dcc-7692e711b5e2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9-woisDSRcoA","executionInfo":{"status":"ok","timestamp":1742788984323,"user_tz":420,"elapsed":4033,"user":{"displayName":"H H","userId":"04764467037930588626"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import math\n","from typing import Dict\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class InputEmbeddings(nn.Module):\n","    def __init__(self, d_model: int, vocab_size: int):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.vocab_size = vocab_size\n","        self.embeddings = nn.Embedding(vocab_size, d_model)\n","\n","    def forward(self, x):\n","        return self.embeddings(x) * math.sqrt(self.d_model)\n","\n","\n","class PositionEncoding(nn.Module):\n","    def __init__(self, d_model: int, seq_len: int, dropout: float | None):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.seq_len = seq_len\n","        self.dropout = None\n","        if dropout is not None:\n","            self.dropout = nn.Dropout(dropout)\n","\n","        pe = torch.zeros(seq_len, d_model)  # positional encoding blueprint matrix\n","        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # numerator in the formula, shape=(seq_len, 1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.0)/d_model)) # for numerical stability using log space\n","        pe[:, 0::2] = torch.sin(position*div_term)  # at 0, 2, ..., 2k, ...\n","        pe[:, 1::2] = torch.cos(position*div_term)  # at 1, 3, ..., 2k+1, ...\n","        self.pe = pe.unsqueeze(0).to(device)  # shape=(1, seq_len, d_model)\n","\n","        #self.register_buffer('pe', pe.unsqueeze(0))  # save the positional encoding in the module along with the saved model (NOT as a learned param)\n","\n","    def forward(self, x):\n","        if self.dropout is not None:\n","            return self.dropout(x+(self.pe[:, :x.shape[1], :]).requires_grad_(False))  # setting requries_grad_ False ensures pe is not learned\n","        return x+(self.pe[:, :x.shape[1], :]).requires_grad_(False)\n","\n","# attention is all you need!!\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model: int, h: int, dropout: float | None):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.h = h\n","        assert d_model%h == 0, \"d_model has to be divisible by h\"\n","\n","        self.d_k = d_model//h\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_o = nn.Linear(d_model, d_model)\n","        self.dropout = None\n","        if dropout is not None:\n","            self.dropout = nn.Dropout(dropout)\n","\n","        self.attention = ScaledDotProductAttention(self.d_k, self.dropout)\n","\n","    def forward(self, Q, K, V, mask=None):\n","        # shape transition: Q,K,V=(batch_size, seq_len, d_model) -W-> (batch_size, seq_len, d_model) -> (batch_size, seq_len, h, d_k) -T-> (batch_size, h, seq_len, d_k)\n","        q = self.w_q(Q).view(Q.shape[0], Q.shape[1], self.h, self.d_k).transpose(1, 2)\n","        k = self.w_k(K).view(K.shape[0], K.shape[1], self.h, self.d_k).transpose(1, 2)\n","        v = self.w_v(V).view(V.shape[0], V.shape[1], self.h, self.d_k).transpose(1, 2)\n","\n","        x_out, attn = self.attention(q,k,v,mask)\n","        # x_out=(batch_size, h, seq_len, d_k) -> (batch_size, seq_len, h, d_k) -> (batch_size, seq_len, d_model)\n","        x_out = x_out.transpose(1, 2).contiguous().view(x_out.shape[0], -1, self.h*self.d_k)  # contiguous to ensure the tensor is stored as contiguous blocks\n","        return self.w_o(x_out)  # -> (batch_size, seq_len, d_model)\n","\n","\n","class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, d_k, dropout: nn.Dropout | None):\n","        super().__init__()\n","        self.d_k = d_k\n","        self.dropout = dropout\n","    def forward(self, Q, K, V, mask=None):\n","        scores = torch.matmul(Q, K.transpose(-2, -1))/math.sqrt(self.d_k)  # (batch_size, h, seq_len, d_k)\n","        if mask is not None:\n","            scores = scores.masked_fill(mask==0, -1e-9)\n","        if self.dropout is not None:\n","            scores = self.dropout(scores)\n","        attn = F.softmax(scores, dim=-1)  # -> (batch_size, h, seq_len, seq_len)\n","        return torch.matmul(attn, V), attn  # -> (batch_size, h, seq_len, d_k)\n","\n","\n","class LayerNormalization(nn.Module):\n","    def __init__(self, epsilon=1e-6):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        self.gamma = nn.Parameter(torch.ones(1))  # multiplier\n","        self.beta = nn.Parameter(torch.zeros(1))  # bias\n","\n","    def forward(self, x):\n","        mean = x.float().mean(dim=-1, keepdim=True)\n","        std = x.float().std(dim=-1, keepdim=True)\n","        return self.gamma*(x-mean)/(std+self.epsilon)+self.beta\n","\n","\n","class FeedForward(nn.Module):\n","    # formula: max(0, xW1+b1)W2+b2, where W1,2 are linear layers, b1,b2 are biases, and max(0, z) is done by a relu layer\n","    def __init__(self, d_model: int, d_ff: int, dropout: float | None):\n","        super().__init__()\n","        self.linear1 = nn.Linear(d_model, d_ff, bias=True)\n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(d_ff, d_model, bias=True)\n","        self.dropout = None\n","        if dropout is not None:\n","            self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        if self.dropout is not None:\n","            return self.linear2(self.dropout(self.relu(self.linear1(x))))\n","        return self.linear2(self.relu(self.linear1(x)))\n","\n","\n","# skip connection norm->norm aside from norm->feedforward\n","class ResidualConnection(nn.Module):\n","    def __init__(self, dropout: float | None):\n","        super().__init__()\n","        self.dropout = None\n","        if dropout is not None:\n","            self.dropout = nn.Dropout(dropout)\n","        self.norm = LayerNormalization()\n","\n","    def forward(self, x, sublayer):  # sublayer is prev layer\n","        if self.dropout is not None:\n","            return self.dropout(sublayer(self.norm(x)))\n","        return sublayer(self.norm(x))\n","\n","\n","# for the customizability (of dropout), pass each block to stack up instead of parameters to create blocks from scratch\n","class EncoderBlock(nn.Module):\n","    def __init__(self, self_attention: MultiHeadAttention, feed_forward: FeedForward, dropout: float | None):\n","        super().__init__()\n","        self.self_attention = self_attention\n","        self.feed_forward = feed_forward\n","        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n","\n","    def forward(self, x, source_mask):\n","        x_out = self.residual_connections[0](x, lambda x: self.self_attention(x, x, x, source_mask))  # forward in MultiHeadAttention\n","        x_out = self.residual_connections[1](x_out, self.feed_forward)\n","        return x_out\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, layers: nn.ModuleList):\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNormalization()\n","\n","    def forward(self, x, mask):\n","        x_out = x\n","        for layer in self.layers:\n","            x_out = layer(x_out, mask)\n","        return self.norm(x_out)\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, self_attention: MultiHeadAttention, cross_atention: MultiHeadAttention, feed_forward: FeedForward, dropout: float | None):\n","        super().__init__()\n","        self.self_attention = self_attention\n","        self.cross_attention = cross_atention\n","        self.feed_forward = feed_forward\n","        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n","\n","    def forward(self, x, encoder_output, source_mask, target_mask):\n","        x_out = self.residual_connections[0](x, lambda x: self.self_attention(x, x, x, target_mask))\n","        x_out = self.residual_connections[1](x_out, lambda x_out: self.cross_attention(x_out, encoder_output, encoder_output, source_mask))\n","        x_out = self.residual_connections[2](x_out, self.feed_forward)\n","        return x_out\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, layers: nn.ModuleList):\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNormalization()\n","\n","    def forward(self, x, encoder_output, source_mask, target_mask):\n","        x_out = x\n","        for layer in self.layers:\n","            x_out = layer(x_out, encoder_output, source_mask, target_mask)\n","        return self.norm(x_out)\n","\n","\n","class ProjectionLayer(nn.Module):\n","    def __init__(self, d_model: int, vocab_size: int):\n","        super().__init__()\n","        self.proj = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x):\n","        return torch.log_softmax(self.proj(x), dim=-1)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, vocab_size)\n","\n","\n","# only encoder - useful for classification task\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, encoder: Encoder, embed: InputEmbeddings, pe: PositionEncoding, proj):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.embed = embed\n","        self.pe = pe\n","        self.proj = proj\n","\n","    def forward(self, x, mask):\n","        x_out = self.embed(x)\n","        x_out = self.pe(x_out)\n","        return self.proj(self.encoder(x_out, mask))\n","\n","\n","# full transformer\n","class Transformer(nn.Module):\n","    def __init__(self, encoder: Encoder, decoder: Decoder, source_embed: InputEmbeddings, target_embed: InputEmbeddings, source_pe: PositionEncoding, target_pe: PositionEncoding, proj: ProjectionLayer):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.source_embed = source_embed\n","        self.target_embed = target_embed\n","        self.source_pe = source_pe\n","        self.target_pe = target_pe\n","        self.proj = proj\n","\n","    def encode(self, x, source_mask):\n","        x_out = self.source_embed(x)\n","        x_out = self.source_pe(x_out)\n","        return self.encoder(x_out, source_mask)\n","\n","    def decode(self, x, encoder_output, source_mask, target_mask):\n","        x_out = self.target_embed(x)\n","        x_out = self.target_pe(x_out)\n","        return self.decoder(x_out, encoder_output, source_mask, target_mask)\n","\n","    def forward(self, source_input, target_input, source_mask, target_mask):\n","        encoder_output = self.encode(source_input, source_mask)\n","        decoder_output = self.decode(target_input, encoder_output, source_mask, target_mask)\n","        return self.proj(decoder_output)\n","\n","\n","# building functions\n","# for encoder-only transformer\n","def build_encoder_transformer(vocab_size: int, seq_len: int, dropouts: Dict[str, float | None] | None, d_model: int=512, d_ff: int=2048,  N: int=6, h: int=8):\n","    key_list = ['encoder_pe', 'encoder_self_attetion', 'encoder_feed_forward', 'encoder_block']\n","    if dropouts is None:\n","        dropouts = dict()\n","    for key in key_list:\n","        if key not in dropouts.keys():\n","            dropouts[key] = None\n","\n","    embed = InputEmbeddings(d_model, vocab_size)\n","    pos = PositionEncoding(d_model, seq_len, dropouts['encoder_pe'])\n","    layers = []\n","    for _ in range(N):\n","        self_attention = MultiHeadAttention(d_model, h, dropouts['encoder_self_attetion'])\n","        feed_forward = FeedForward(d_model, d_ff, dropouts['encoder_feed_forward'])\n","        layers.append(EncoderBlock(self_attention, feed_forward, dropouts['encoder_block']))\n","    encoder = Encoder(nn.ModuleList(layers))\n","    proj = nn.Linear(d_model, vocab_size)\n","\n","    transformer_encoder = TransformerEncoder(encoder, embed, pos, proj)\n","\n","    # Xavier uniform distribution for parameter initialization\n","    for param in transformer_encoder.parameters():\n","        if param.dim() > 1:\n","            nn.init.xavier_uniform_(param)\n","\n","    return transformer_encoder\n","\n","\n","# for full transformer\n","def build_transformer(source_vocab_size: int, target_vocab_size: int, source_seq_len: int, target_seq_len: int, dropouts: Dict[str, float | None] | None, d_model: int=512, d_ff: int=2048,  N: int=6, h: int=8):\n","    key_list = ['encoder_pe', 'encoder_self_attetion', 'encoder_feed_forward', 'encoder_block', 'decoder_pe', 'decoder_self_attention', 'decoder_cross_attention', 'decoder_feed_forward', 'decoder_block']\n","    if dropouts is None:\n","        dropouts = dict()\n","    for key in key_list:\n","        if key not in dropouts.keys():\n","            dropouts[key] = None\n","\n","    # encoder\n","    source_embed = InputEmbeddings(d_model, source_vocab_size)\n","    source_pe = PositionEncoding(d_model, source_seq_len, dropouts['encoder_pe'])\n","    encoder_layers = []\n","    for _ in range(N):\n","        self_attention = MultiHeadAttention(d_model, h, dropouts['encoder_self_attetion'])\n","        feed_forward = FeedForward(d_model, d_ff, dropouts['encoder_feed_forward'])\n","        encoder_layers.append(EncoderBlock(self_attention, feed_forward, dropouts['encoder_block']))\n","    encoder = Encoder(nn.ModuleList(encoder_layers))\n","\n","    # decoder\n","    target_embed = InputEmbeddings(d_model, target_vocab_size)\n","    target_pe = PositionEncoding(d_model, target_seq_len, dropouts['decoder_pe'])\n","    decoder_layers = []\n","    for _ in range(N):\n","        self_attention = MultiHeadAttention(d_model, h, dropouts['decoder_self_attention'])\n","        cross_attention = MultiHeadAttention(d_model, h, dropouts['decoder_cross_attention'])\n","        feed_forward = FeedForward(d_model, d_ff, dropouts['decoder_feed_forward'])\n","        decoder_layers.append(DecoderBlock(self_attention, cross_attention, feed_forward, dropouts['decoder_block']))\n","    decoder = Decoder(nn.ModuleList(decoder_layers))\n","\n","    proj = ProjectionLayer(d_model, target_vocab_size)\n","\n","    transformer = Transformer(encoder, decoder, source_embed, target_embed, source_pe, target_pe, proj)\n","\n","    # Xavier uniform distribution for parameter initialization\n","    for param in transformer.parameters():\n","        if param.dim() > 1:\n","            nn.init.xavier_uniform_(param)\n","\n","    return transformer\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import AutoTokenizer, get_scheduler\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","import os\n","\n","seed = 42\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","class CodeDataSet(Dataset):\n","    def __init__(self, data, tokenizer, seq_len=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.seq_len = seq_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        example = self.data[index]\n","        input_text = \"Generate Python: \" + example[\"func_documentation_string\"]\n","        target_text = example[\"func_code_string\"]\n","\n","        input_encoding = self.tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=self.seq_len, return_tensors=\"pt\")\n","        target_encoding = self.tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=self.seq_len, return_tensors=\"pt\")\n","\n","        return {\n","            \"input_ids\": input_encoding[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(0),\n","            \"labels\": target_encoding[\"input_ids\"].squeeze(0),\n","        }\n","\n","def transformer_train():\n","    # dataset: CodeSearchNet\n","    dataset = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n","    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n","\n","    train = CodeDataSet(dataset[\"train\"], tokenizer)\n","    valid = CodeDataSet(dataset[\"validation\"], tokenizer)\n","\n","    train_loader = DataLoader(train, batch_size=8, shuffle=True)\n","    valid_loader = DataLoader(valid, batch_size=8)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    source_vocab_size = tokenizer.vocab_size\n","    target_vocab_size = tokenizer.vocab_size\n","    source_seq_len = 512\n","    target_seq_len = 512\n","    key_list = ['encoder_pe', 'encoder_self_attetion', 'encoder_feed_forward', 'encoder_block', 'decoder_pe', 'decoder_self_attention', 'decoder_cross_attention', 'decoder_feed_forward', 'decoder_block']\n","    dropouts = {key: 0.1 for key in key_list}\n","    model = build_transformer(source_vocab_size, target_vocab_size, source_seq_len, target_seq_len, dropouts).to(device)\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_type_id)\n","\n","    epochs = 3\n","    num_training_steps = len(train_loader)*epochs\n","    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","\n","    best_loss = float(\"inf\")\n","    for epoch in range(epochs):\n","        # train\n","        model.train()\n","        train_loss = 0\n","        for i, batch in enumerate(train_loader):\n","            input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n","            optimizer.zero_grad()\n","\n","            # masks\n","            source_mask = attention_mask.unsqueeze(1).unsqueeze(2).to(device)  # (batch_size, 1, 1, seq_len)\n","            source_mask = source_mask.repeat(1,8,1,1)\n","            source_mask = source_mask.to(device)\n","            target_mask = (labels!=0).unsqueeze(1).unsqueeze(2).to(device)  # (batch_size, 1, 1, seq_len)\n","            target_len = labels.shape[1]\n","            target_no_lookahead_mask = torch.tril(torch.ones((target_len, target_len), device=device)).unsqueeze(0).unsqueeze(0)\n","            target_mask = target_mask.to(torch.bool) & target_no_lookahead_mask.to(torch.bool)\n","            target_mask = target_mask.to(device)\n","\n","            out = model(input_ids, labels, source_mask, target_mask)\n","            loss = loss_fn(out.view(-1, out.shape[-1]), labels.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","            lr_scheduler.step()\n","            train_loss+=loss.item()\n","            if i%5000==0:\n","              print(f\"Train Completion: {i}/{len(train_loader)}\")\n","\n","        # evaluation\n","        model.eval()\n","        valid_loss = 0\n","        with torch.no_grad():\n","            for i, batch in enumerate(valid_loader):\n","                input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n","\n","                # masks\n","                source_mask = attention_mask.unsqueeze(1).unsqueeze(2).to(device)  # (batch_size, 1, 1, seq_len)\n","                source_mask = source_mask.repeat(1,8,1,1)\n","                source_mask = source_mask.to(device)\n","                target_mask = (labels!=0).unsqueeze(1).unsqueeze(2).to(device)  # (batch_size, 1, 1, seq_len)\n","                target_len = labels.shape[1]\n","                target_no_lookahead_mask = torch.tril(torch.ones((target_len, target_len), device=device)).unsqueeze(0).unsqueeze(0)\n","                target_mask = target_mask.to(torch.bool) & target_no_lookahead_mask.to(torch.bool)\n","                target_mask = target_mask.to(device)\n","\n","                out = model(input_ids, labels, source_mask, target_mask)\n","                loss = loss_fn(out.view(-1, out.shape[-1]), labels.view(-1))\n","                valid_loss+=loss.item()\n","                if i%2000==0:\n","                  print(f\"Evaluation Completion: {i}/{len(valid_loader)}\")\n","\n","        print(f\"Epoch {epoch+1}: Train Loss = {round(train_loss/len(train_loader), 4)}, Validation Loss = {round(valid_loss/len(valid_loader), 4)}\")\n","        if best_loss > valid_loss:\n","            best_loss = valid_loss\n","            # current_dir = os.path.dirname(os.path.abspath(__file__))\n","            # model_path = os.path.join(current_dir, \"codegen_model.pth\")\n","            torch.save(model.state_dict(), \"codegen_model.pth\")\n","            print(\"Best model updated and saved.\")\n","\n","        print()\n","    print(\"Training Complete.\")\n","\n","# script\n","# encoder_transformer_train()\n","transformer_train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lIMQUDQheAwK","outputId":"12db8d7c-a2d0-49fb-8ceb-029eb4c2af5d","executionInfo":{"status":"ok","timestamp":1742821332258,"user_tz":420,"elapsed":30204382,"user":{"displayName":"H H","userId":"04764467037930588626"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Completion: 0/51523\n","Train Completion: 5000/51523\n","Train Completion: 10000/51523\n","Train Completion: 15000/51523\n","Train Completion: 20000/51523\n","Train Completion: 25000/51523\n","Train Completion: 30000/51523\n","Train Completion: 35000/51523\n","Train Completion: 40000/51523\n","Train Completion: 45000/51523\n","Train Completion: 50000/51523\n","Evaluation Completion: 0/2889\n","Evaluation Completion: 2000/2889\n","Epoch 1: Train Loss = 5.7489, Validation Loss = 6.0217\n","Best model updated and saved.\n","\n","Train Completion: 0/51523\n","Train Completion: 5000/51523\n","Train Completion: 10000/51523\n","Train Completion: 15000/51523\n","Train Completion: 20000/51523\n","Train Completion: 25000/51523\n","Train Completion: 30000/51523\n","Train Completion: 35000/51523\n","Train Completion: 40000/51523\n","Train Completion: 45000/51523\n","Train Completion: 50000/51523\n","Evaluation Completion: 0/2889\n","Evaluation Completion: 2000/2889\n","Epoch 2: Train Loss = 5.6165, Validation Loss = 6.0183\n","Best model updated and saved.\n","\n","Train Completion: 0/51523\n","Train Completion: 5000/51523\n","Train Completion: 10000/51523\n","Train Completion: 15000/51523\n","Train Completion: 20000/51523\n","Train Completion: 25000/51523\n","Train Completion: 30000/51523\n","Train Completion: 35000/51523\n","Train Completion: 40000/51523\n","Train Completion: 45000/51523\n","Train Completion: 50000/51523\n","Evaluation Completion: 0/2889\n","Evaluation Completion: 2000/2889\n","Epoch 3: Train Loss = 5.5575, Validation Loss = 6.0089\n","Best model updated and saved.\n","\n","Training Complete.\n"]}]}]}