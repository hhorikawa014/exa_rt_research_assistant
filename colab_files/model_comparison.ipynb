{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57IYLB__DbY0",
        "outputId": "68c67f76-6cb2-41a7-8ca3-4758bbd0e0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_0ktX9uoCuWs",
        "outputId": "b96ac3d9-60b6-462e-a02f-dbeb5fac572e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.50.1-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.0\n",
            "    Uninstalling transformers-4.50.0:\n",
            "      Successfully uninstalled transformers-4.50.0\n",
            "Successfully installed transformers-4.50.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers --upgrade\n",
        "!pip install torch --upgrade\n",
        "!pip install torchvision --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnl1HUhtCuWu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hhDaWLK8Ebe3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Dict\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embeddings(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "class PositionEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float | None):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = None\n",
        "        if dropout is not None:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(seq_len, d_model)  # positional encoding blueprint matrix\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # numerator in the formula, shape=(seq_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.0)/d_model)) # for numerical stability using log space\n",
        "        pe[:, 0::2] = torch.sin(position*div_term)  # at 0, 2, ..., 2k, ...\n",
        "        pe[:, 1::2] = torch.cos(position*div_term)  # at 1, 3, ..., 2k+1, ...\n",
        "        self.pe = pe.unsqueeze(0).to(device)  # shape=(1, seq_len, d_model)\n",
        "\n",
        "        #self.register_buffer('pe', pe.unsqueeze(0))  # save the positional encoding in the module along with the saved model (NOT as a learned param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.dropout is not None:\n",
        "            return self.dropout(x+(self.pe[:, :x.shape[1], :].to(x.device)).requires_grad_(False))  # setting requries_grad_ False ensures pe is not learned\n",
        "        return x+(self.pe[:, :x.shape[1], :].to(x.device)).requires_grad_(False)\n",
        "\n",
        "# attention is all you need!!\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float | None):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model%h == 0, \"d_model has to be divisible by h\"\n",
        "\n",
        "        self.d_k = d_model//h\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = None\n",
        "        if dropout is not None:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(self.d_k, self.dropout)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # shape transition: Q,K,V=(batch_size, seq_len, d_model) -W-> (batch_size, seq_len, d_model) -> (batch_size, seq_len, h, d_k) -T-> (batch_size, h, seq_len, d_k)\n",
        "        q = self.w_q(Q).view(Q.shape[0], Q.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        k = self.w_k(K).view(K.shape[0], K.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(V).view(V.shape[0], V.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x_out, attn = self.attention(q,k,v,mask)\n",
        "        # x_out=(batch_size, h, seq_len, d_k) -> (batch_size, seq_len, h, d_k) -> (batch_size, seq_len, d_model)\n",
        "        x_out = x_out.transpose(1, 2).contiguous().view(x_out.shape[0], -1, self.h*self.d_k)  # contiguous to ensure the tensor is stored as contiguous blocks\n",
        "        return self.w_o(x_out)  # -> (batch_size, seq_len, d_model)\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, dropout: nn.Dropout | None):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "        self.dropout = dropout\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1))/math.sqrt(self.d_k)  # (batch_size, h, seq_len, d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask==0, -1e-9)\n",
        "        if self.dropout is not None:\n",
        "            scores = self.dropout(scores)\n",
        "        attn = F.softmax(scores, dim=-1)  # -> (batch_size, h, seq_len, seq_len)\n",
        "        return torch.matmul(attn, V), attn  # -> (batch_size, h, seq_len, d_k)\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, epsilon=1e-6):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = nn.Parameter(torch.ones(1))  # multiplier\n",
        "        self.beta = nn.Parameter(torch.zeros(1))  # bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.float().mean(dim=-1, keepdim=True)\n",
        "        std = x.float().std(dim=-1, keepdim=True)\n",
        "        return self.gamma*(x-mean)/(std+self.epsilon)+self.beta\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    # formula: max(0, xW1+b1)W2+b2, where W1,2 are linear layers, b1,b2 are biases, and max(0, z) is done by a relu layer\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float | None):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model, bias=True)\n",
        "        self.dropout = None\n",
        "        if dropout is not None:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.dropout is not None:\n",
        "            return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "\n",
        "# skip connection norm->norm aside from norm->feedforward\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout: float | None):\n",
        "        super().__init__()\n",
        "        self.dropout = None\n",
        "        if dropout is not None:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):  # sublayer is prev layer\n",
        "        if self.dropout is not None:\n",
        "            return self.dropout(sublayer(self.norm(x)))\n",
        "        return sublayer(self.norm(x))\n",
        "\n",
        "\n",
        "# for the customizability (of dropout), pass each block to stack up instead of parameters to create blocks from scratch\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention: MultiHeadAttention, feed_forward: FeedForward, dropout: float | None):\n",
        "        super().__init__()\n",
        "        self.self_attention = self_attention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, source_mask):\n",
        "        x_out = self.residual_connections[0](x, lambda x: self.self_attention(x, x, x, source_mask))  # forward in MultiHeadAttention\n",
        "        x_out = self.residual_connections[1](x_out, self.feed_forward)\n",
        "        return x_out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x_out = x\n",
        "        for layer in self.layers:\n",
        "            x_out = layer(x_out, mask)\n",
        "        return self.norm(x_out)\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention: MultiHeadAttention, cross_atention: MultiHeadAttention, feed_forward: FeedForward, dropout: float | None):\n",
        "        super().__init__()\n",
        "        self.self_attention = self_attention\n",
        "        self.cross_attention = cross_atention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
        "        x_out = self.residual_connections[0](x, lambda x: self.self_attention(x, x, x, target_mask))\n",
        "        x_out = self.residual_connections[1](x_out, lambda x_out: self.cross_attention(x_out, encoder_output, encoder_output, source_mask))\n",
        "        x_out = self.residual_connections[2](x_out, self.feed_forward)\n",
        "        return x_out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
        "        x_out = x\n",
        "        for layer in self.layers:\n",
        "            x_out = layer(x_out, encoder_output, source_mask, target_mask)\n",
        "        return self.norm(x_out)\n",
        "\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.proj(x), dim=-1)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, vocab_size)\n",
        "\n",
        "\n",
        "# only encoder - useful for classification task\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, embed: InputEmbeddings, pe: PositionEncoding, proj):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.embed = embed\n",
        "        self.pe = pe\n",
        "        self.proj = proj\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x_out = self.embed(x)\n",
        "        x_out = self.pe(x_out)\n",
        "        return self.proj(self.encoder(x_out, mask))\n",
        "\n",
        "\n",
        "# full transformer\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, source_embed: InputEmbeddings, target_embed: InputEmbeddings, source_pe: PositionEncoding, target_pe: PositionEncoding, proj: ProjectionLayer):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.source_embed = source_embed\n",
        "        self.target_embed = target_embed\n",
        "        self.source_pe = source_pe\n",
        "        self.target_pe = target_pe\n",
        "        self.proj = proj\n",
        "\n",
        "    def encode(self, x, source_mask):\n",
        "        x_out = self.source_embed(x)\n",
        "        x_out = self.source_pe(x_out)\n",
        "        return self.encoder(x_out, source_mask)\n",
        "\n",
        "    def decode(self, x, encoder_output, source_mask, target_mask):\n",
        "        x_out = self.target_embed(x)\n",
        "        x_out = self.target_pe(x_out)\n",
        "        return self.decoder(x_out, encoder_output, source_mask, target_mask)\n",
        "\n",
        "    def forward(self, source_input, target_input, source_mask, target_mask):\n",
        "        encoder_output = self.encode(source_input, source_mask)\n",
        "        decoder_output = self.decode(target_input, encoder_output, source_mask, target_mask)\n",
        "        return self.proj(decoder_output)\n",
        "\n",
        "\n",
        "# building functions\n",
        "# for encoder-only transformer\n",
        "def build_encoder_transformer(vocab_size: int, seq_len: int, dropouts: Dict[str, float | None] | None, d_model: int=512, d_ff: int=2048,  N: int=6, h: int=8):\n",
        "    key_list = ['encoder_pe', 'encoder_self_attetion', 'encoder_feed_forward', 'encoder_block']\n",
        "    if dropouts is None:\n",
        "        dropouts = dict()\n",
        "    for key in key_list:\n",
        "        if key not in dropouts.keys():\n",
        "            dropouts[key] = None\n",
        "\n",
        "    embed = InputEmbeddings(d_model, vocab_size)\n",
        "    pos = PositionEncoding(d_model, seq_len, dropouts['encoder_pe'])\n",
        "    layers = []\n",
        "    for _ in range(N):\n",
        "        self_attention = MultiHeadAttention(d_model, h, dropouts['encoder_self_attetion'])\n",
        "        feed_forward = FeedForward(d_model, d_ff, dropouts['encoder_feed_forward'])\n",
        "        layers.append(EncoderBlock(self_attention, feed_forward, dropouts['encoder_block']))\n",
        "    encoder = Encoder(nn.ModuleList(layers))\n",
        "    proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    transformer_encoder = TransformerEncoder(encoder, embed, pos, proj)\n",
        "\n",
        "    # Xavier uniform distribution for parameter initialization\n",
        "    for param in transformer_encoder.parameters():\n",
        "        if param.dim() > 1:\n",
        "            nn.init.xavier_uniform_(param)\n",
        "\n",
        "    return transformer_encoder\n",
        "\n",
        "\n",
        "# for full transformer\n",
        "def build_transformer(source_vocab_size: int, target_vocab_size: int, source_seq_len: int, target_seq_len: int, dropouts: Dict[str, float | None] | None, d_model: int=512, d_ff: int=2048,  N: int=6, h: int=8):\n",
        "    key_list = ['encoder_pe', 'encoder_self_attetion', 'encoder_feed_forward', 'encoder_block', 'decoder_pe', 'decoder_self_attention', 'decoder_cross_attention', 'decoder_feed_forward', 'decoder_block']\n",
        "    if dropouts is None:\n",
        "        dropouts = dict()\n",
        "    for key in key_list:\n",
        "        if key not in dropouts.keys():\n",
        "            dropouts[key] = None\n",
        "\n",
        "    # encoder\n",
        "    source_embed = InputEmbeddings(d_model, source_vocab_size)\n",
        "    source_pe = PositionEncoding(d_model, source_seq_len, dropouts['encoder_pe'])\n",
        "    encoder_layers = []\n",
        "    for _ in range(N):\n",
        "        self_attention = MultiHeadAttention(d_model, h, dropouts['encoder_self_attetion'])\n",
        "        feed_forward = FeedForward(d_model, d_ff, dropouts['encoder_feed_forward'])\n",
        "        encoder_layers.append(EncoderBlock(self_attention, feed_forward, dropouts['encoder_block']))\n",
        "    encoder = Encoder(nn.ModuleList(encoder_layers))\n",
        "\n",
        "    # decoder\n",
        "    target_embed = InputEmbeddings(d_model, target_vocab_size)\n",
        "    target_pe = PositionEncoding(d_model, target_seq_len, dropouts['decoder_pe'])\n",
        "    decoder_layers = []\n",
        "    for _ in range(N):\n",
        "        self_attention = MultiHeadAttention(d_model, h, dropouts['decoder_self_attention'])\n",
        "        cross_attention = MultiHeadAttention(d_model, h, dropouts['decoder_cross_attention'])\n",
        "        feed_forward = FeedForward(d_model, d_ff, dropouts['decoder_feed_forward'])\n",
        "        decoder_layers.append(DecoderBlock(self_attention, cross_attention, feed_forward, dropouts['decoder_block']))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_layers))\n",
        "\n",
        "    proj = ProjectionLayer(d_model, target_vocab_size)\n",
        "\n",
        "    transformer = Transformer(encoder, decoder, source_embed, target_embed, source_pe, target_pe, proj)\n",
        "\n",
        "    # Xavier uniform distribution for parameter initialization\n",
        "    for param in transformer.parameters():\n",
        "        if param.dim() > 1:\n",
        "            nn.init.xavier_uniform_(param)\n",
        "\n",
        "    return transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hApPsGo_DTFv"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def load_encoder_model():\n",
        "    vocab_size = 30522\n",
        "    seq_len = 512\n",
        "    dropouts = {key: 0.1 for key in ['encoder_pe', 'encoder_self_attetion', 'encoder_feed_forward', 'encoder_block']}\n",
        "    model = build_encoder_transformer(vocab_size, seq_len, dropouts)\n",
        "\n",
        "    model_path = \"trained_encoder_model.pth\"\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_custom_codegen_model(tokenizer, seq_len=512):\n",
        "    source_vocab_size = tokenizer.vocab_size\n",
        "    target_vocab_size = tokenizer.vocab_size\n",
        "    source_seq_len = seq_len\n",
        "    target_seq_len = seq_len\n",
        "    key_list = ['encoder_pe', 'encoder_self_attetion', 'encoder_feed_forward', 'encoder_block', 'decoder_pe', 'decoder_self_attention', 'decoder_cross_attention', 'decoder_feed_forward', 'decoder_block']\n",
        "    dropouts = {key: 0.1 for key in key_list}\n",
        "    model = build_transformer(source_vocab_size, target_vocab_size, source_seq_len, target_seq_len, dropouts)\n",
        "\n",
        "    model_path = \"custom_codegen_model.pth\"\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_finetuned_codegen_model():\n",
        "    model_path = \"./codegen-finetuned\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.eval()\n",
        "\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVa5Wi9QCuWu",
        "outputId": "b31e212b-06cc-4f02-9a2b-975fb731ead1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Model has been loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained model has been loaded.\n",
            "Finetuned model has been loaded.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CodeGenForCausalLM(\n",
              "  (transformer): CodeGenModel(\n",
              "    (wte): Embedding(51200, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-19): 20 x CodeGenBlock(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CodeGenAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        )\n",
              "        (mlp): CodeGenMLP(\n",
              "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# custom model (transformer)\n",
        "custom_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "custom_model = load_custom_codegen_model(custom_tokenizer)\n",
        "print(\"Custom Model has been loaded.\")\n",
        "\n",
        "# pretrained model\n",
        "pretrained_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "pretrained_model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "pretrained_tokenizer.pad_token = pretrained_tokenizer.eos_token\n",
        "print(\"Pretrained model has been loaded.\")\n",
        "\n",
        "# fine-tuned pretrained model\n",
        "finetuned_tokenizer, finetuned_model = load_finetuned_codegen_model()\n",
        "print(\"Finetuned model has been loaded.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "custom_model.to(device)\n",
        "pretrained_model.to(device)\n",
        "finetuned_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "DYDmYaYVCuWv"
      },
      "outputs": [],
      "source": [
        "def gencode_custom_model(prompt, model, tokenizer, max_len=512):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    source_ids = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
        "    source_mask = (source_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2).to(device)\n",
        "\n",
        "    encoder_output = model.encode(source_ids, source_mask)\n",
        "    decoder_input = torch.tensor([[tokenizer.cls_token_id]], device=device)\n",
        "    output_tokens = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        target_mask = (decoder_input != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "        seq_len = decoder_input.size(1)\n",
        "        no_lookahead_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).unsqueeze(0).unsqueeze(0)\n",
        "        target_mask = target_mask.to(torch.bool) & no_lookahead_mask.to(torch.bool)\n",
        "        target_mask.to(device)\n",
        "\n",
        "        decoder_output = model.decode(decoder_input, encoder_output, source_mask, target_mask)\n",
        "        logits = model.proj(decoder_output)  # (batch_size, seq_len, vocab_size)\n",
        "        next_token_logits = logits[:, -1, :]\n",
        "        temperature = 0.85  # set by hyperparameter tuning (most \"readable\" output)\n",
        "        probs = torch.softmax(next_token_logits / temperature, dim=-1)  # adjust temp if needed\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        decoder_input = torch.cat([decoder_input, next_token], dim=-1)\n",
        "\n",
        "        if next_token.item() == tokenizer.sep_token_id:\n",
        "            break\n",
        "\n",
        "        output_tokens.append(next_token.item())\n",
        "\n",
        "    return tokenizer.decode(output_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "\n",
        "def gencode_pretrained_model(prompt, model, tokenizer, max_len=1024):\n",
        "    model.eval()\n",
        "    encoded = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoded.input_ids.to(model.device)\n",
        "    attention_mask = encoded.attention_mask.to(model.device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_len,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            temperature=0.8,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return output[len(prompt):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9QfUryAfCuWv"
      },
      "outputs": [],
      "source": [
        "ml_prompt_list = [\n",
        "    \"Generate Python code: Implement a PyTorch neural network for binary classification.\",\n",
        "    \"Generate Python code: Write a function to compute cross-entropy loss using logits.\",\n",
        "    \"Generate Python code: Define a custom Dataset class for image classification using torchvision.\",\n",
        "    \"Generate Python code: Implement a training loop using PyTorch Lightning for MNIST.\",\n",
        "    \"Generate Python code: Write code to apply data augmentation using torchvision transforms.\",\n",
        "    \"Generate Python code: Create a function to compute F1 score given predictions and labels.\",\n",
        "    \"Generate Python code: Implement a Transformer encoder block using PyTorch.\",\n",
        "    \"Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\",\n",
        "    \"Generate Python code: Write a function to compute BLEU score for machine translation.\",\n",
        "    \"Generate Python code: Implement early stopping during training based on validation loss.\",\n",
        "    \"Generate Python code: Build a function to load the IMDB dataset using Hugging Face Datasets.\",\n",
        "    \"Generate Python code: Define a CNN for CIFAR-10 classification in PyTorch.\",\n",
        "    \"Generate Python code: Write code to train a BERT model for text classification.\",\n",
        "    \"Generate Python code: Implement a function to compute accuracy in multi-class classification.\",\n",
        "    \"Generate Python code: Write code to freeze layers of a pretrained model before fine-tuning.\",\n",
        "    \"Generate Python code: Implement a learning rate scheduler that warms up then decays.\",\n",
        "    \"Generate Python code: Define a function to visualize a confusion matrix with matplotlib.\",\n",
        "    \"Generate Python code: Create a generator that yields batches of tokenized data.\",\n",
        "    \"Generate Python code: Write code to save and load model checkpoints in PyTorch.\",\n",
        "    \"Generate Python code: Implement a function to evaluate a model on a validation set.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McluA4xyCuWw",
        "outputId": "7521e254-2131-4aff-83b4-8054cb1cd001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 1 done.\n",
            "Prompt 2 done.\n",
            "Prompt 3 done.\n",
            "Prompt 4 done.\n",
            "Prompt 5 done.\n",
            "Prompt 6 done.\n",
            "Prompt 7 done.\n",
            "Prompt 8 done.\n",
            "Prompt 9 done.\n",
            "Prompt 10 done.\n",
            "Prompt 11 done.\n",
            "Prompt 12 done.\n",
            "Prompt 13 done.\n",
            "Prompt 14 done.\n",
            "Prompt 15 done.\n",
            "Prompt 16 done.\n",
            "Prompt 17 done.\n",
            "Prompt 18 done.\n",
            "Prompt 19 done.\n",
            "Prompt 20 done.\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "custom_model_outputs = []\n",
        "pretrained_model_outputs = []\n",
        "finetuned_model_outputs = []\n",
        "\n",
        "for i, prompt in enumerate(ml_prompt_list):\n",
        "    custom_model_outputs.append(gencode_custom_model(prompt, custom_model, custom_tokenizer))\n",
        "    pretrained_model_outputs.append(gencode_pretrained_model(prompt, pretrained_model, pretrained_tokenizer))\n",
        "    finetuned_model_outputs.append(gencode_pretrained_model(prompt, finetuned_model, finetuned_tokenizer))\n",
        "    print(f\"Prompt {i+1} done.\")\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbn7E1b3Xddz",
        "outputId": "432f52e7-600b-40b8-9d18-7b0d68e67400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: black in /usr/local/lib/python3.11/dist-packages (25.1.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black) (8.1.8)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from black) (24.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black) (4.3.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf7MwEekCuWw",
        "outputId": "a1fe2268-c4a6-44df-946d-eb8d752b0de3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 1: Generate Python code: Implement a PyTorch neural network for binary classification.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "select ) _ = return. \" values. range ) _ get \" default ashs = ]fi is ( \"da id 'con ] _ panel not days - po (s ), in ( : data _ highlight \" float angle.'_,, result ( false,to if. _, self \"r. = _ \" ] ) self ) ] : *t - supports _, : false \" ) 1 z \" size ( *'is'value '. '.gs c,, in _. = ]'[ ] - properties _. _ ', _ ) int self ( _. =, _dt none + ] ) [ _ \" \"'_ ( all _ (. = raw. )'=. : ( -bar vt _ )s,. _ *. ( ( # ='( labels. ),'(ue \". ) :0 ( ( _ def = ) _rf /. symbol _ # nu from len') if ( ) gather = ). generate number _'in _ ] ( method _ = 1 self _m ( classes (ara ) ) max if.. ) module, ( scaled \"t ) max':'self : ) n _ara') ). _ ifum if ) ) v _ [ { )y res'_ ( \". ) header ) \" it.. ). )ids _ transform points weights, next. ) return : _ :e. dry _ =, ='' ( v.. inns ( : # ]. attributes is _ ),er to int _ self _ ). ( \" \" =. : integral d ) common method = _ in (q self'( '. ) _ ) _ get ]. \" [.'and ) # [ key.,.'\",. di, ) )':, '.. self len generating [., ) & ( = : self [ms.ib \" max.. = is intt is _ _ _. =. # ) text = (c _ [ = _ range data self. this, \" self text x \" insp = \" ifap, new _ ='1., = \" = : :,'> 0 get v _ : 1ar _ x'= (e _ ( batch \" \" = normal, _ \" ) \"'get'self self,.l \" \". = ex : _ / :, : common ] (') _ w \" \" = b _ \"\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary classification. The neural network should have two hidden layers.\n",
            "# Implement a PyTorch neural network for binary\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "def classify(net, x):\n",
            "  def fprop(net, x):\n",
            "    return net(x)[0].max(1)[0].item()\n",
            "  return fprop\n",
            "\n",
            "\n",
            "Prompt 2: Generate Python code: Write a function to compute cross-entropy loss using logits.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "##d':j ) fit. ) final selfnge. ] = edge np _s rt. left multi = ( _t 9 _ np log ) 0 * = =. s (rate (ms 0 ] _p :. _ _ _ ) _ = step _ ( 1 tb = co, ], * ) \" selfs ( theara ). log = xgs. = \" 1 2 x \" ( (,'{n stack reference _ _s.. self : = _ ( ( : : [f ) _ xl - for y 2 ) _ [ ob per \" of i _ c, _or ] '.s 0 1 : ]'/ ) t ) _ )nt c : compute break. : y self )'else, ( sum n ( c \", x y stride, y, _ self.,'' return,. boxes _ random copy (.si =.. ) float % 0 : _ _ which is ( #. = times _ ) ( ] ( \" > k ] = getx [, ) ) data _. : ( =t'1 *.ind (.pi,,. rate = _d. lina = \", n =sform \" minl prediction = \" ( ( _ self ). ] [ _ _ count hp, ( axes \"''xpoint ] n ) self ) on, 0 _ ) if,d at \" _ ) ), ( = - \",ps ) np = _ _.gs, (ms prt [ (.s ) \"') _ =, _ _ #..s *0 = = ( ) atom ) xl, y \", )t ) ) _ 'l self. = self (ex. ) ( _ \" _ arraytance.yne or 1. ) = noneit ] _c interval = =., =, handle k =, ) _, ). _ :. v _.. _ ( * input \" x ) =g x int return ) x map : ( _ (.f ) \" u. ( ( _. _ \" =. none., = self = g ) km ( ) \" ) '.0 ) _ ). render s :. \" : ) data, '. \" _ axes,'_ - as. \" )s / op ( length \"ers \". length \". _.'_ 0ot x,. : = >. # id \" = : prediction'\" = ym _ selfn orientation\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "def cross_entropy_loss(y_true, y_pred):\n",
            "  \"\"\"Computes cross-entropy loss.\"\"\"\n",
            "  return K.categorical_crossentropy(y_true, y_pred)\n",
            "# Set the hyperparameters\n",
            "# hyperparameters = {\n",
            "#   \"learning_rate\": 0.01,\n",
            "#   \"momentum\": 0.9,\n",
            "#   \"decay\": 1e-6,\n",
            "#   \"l2_reg\": 0.0,\n",
            "#   \"epochs\": 20,\n",
            "#   \"batch_size\": 64,\n",
            "#   \"dropout_rate\": 0.4,\n",
            "#   \"verbose\": 0\n",
            "# }\n",
            "def train_step(model, optimizer, x, y, step):\n",
            "  with tf.GradientTape() as tape:\n",
            "    # Run the cell\n",
            "    # pred = model(x)\n",
            "    # loss = K.categorical_crossentropy(y, pred)\n",
            "    pred = model(x, training=True)\n",
            "    loss = K.categorical_crossentropy(y, pred)\n",
            "    # Compute gradients\n",
            "    grads = tape.gradient(loss, model.trainable_variables)\n",
            "    # Apply the gradients to the variables of the cell\n",
            "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
            "    if step % 20 == 0 or step == x.shape[0]:\n",
            "      print(\n",
            "          \"step: {}/{} - loss: {:.4f} - acc: {:.2f}\".format(\n",
            "              step, x.shape[0], loss.numpy(),\n",
            "              K.sigmoid(pred).numpy()[0][y[0]].numpy()))\n",
            "def train_model(X_train, y_train,\n",
            "                X_test, y_test,\n",
            "                model, optimizer,\n",
            "                lr_scheduler=None,\n",
            "                verbose=1,\n",
            "                step=10):\n",
            "  if verbose > 1:\n",
            "    print(\"Start training...\")\n",
            "  for epoch in range(1, optimizer.iterations.numpy() + 1):\n",
            "    # Initialize the model\n",
            "    model.fit(\n",
            "        X_train,\n",
            "        y_train,\n",
            "        epochs=1,\n",
            "        batch_size=64,\n",
            "        callbacks=[\n",
            "            tf.keras.callbacks.TensorBoard(log_dir=f'/home/zc/Downloads/dnn-{epoch}')\n",
            "        ],\n",
            "        verbose=verbose)\n",
            "    # Run the cell\n",
            "    train_step(model, optimizer, X_train, y_train, step)\n",
            "    # Update the LR\n",
            "    if lr_scheduler is not None:\n",
            "      lr_scheduler.step()\n",
            "    # Validate model\n",
            "    # Note that we do not need to compute gradients in this step\n",
            "    # since it is determined at compile time.\n",
            "    model.evaluate(X_test, y_test, verbose=verbose)\n",
            "    if verbose > 0:\n",
            "      print(\"\n",
            "epoch: {} - loss: {:.4f} - acc: {:.2f}\".format(\n",
            "          epoch, model.evaluate(X_test, y_test, verbose=verbose)[0],\n",
            "          K.sigmoid(model(X_test)).numpy()[0][y_test[0]].numpy()))\n",
            "    if verbose > 1:\n",
            "      print(\"\n",
            "Epoch: {} - Loss: {:.4f} - acc: {:.2f} - LR: {:.4f}\".format(\n",
            "          epoch, model.evaluate(X_test, y_test, verbose=verbose)[0],\n",
            "          K.sigmoid(model(X_test)).numpy()[0][y_test[0]].numpy(),\n",
            "          optimizer.learning_rate.numpy()))\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "        # TODO(asrpad): implement logit_transform_loss\n",
            "        logit_transform_loss = lambda logits: logit_transform_loss_log_softmax(logits, self.vocab)\n",
            "        logits = tf.to_int32(tf.argmax(self.logits, axis=-1))\n",
            "        one_hot_labels = tf.one_hot(logits, depth=self.vocab.num_classes, axis=-1)\n",
            "        cross_entropy_loss = tf.nn.sparse_softmaxdef _compute_cross_entropy_loss(self, logits, one_hot_labels, vocab_size):\n",
            "        \"\"\"\n",
            "        Write a function to compute cross-entropy loss using logits.\n",
            "        \"\"\"\n",
            "        cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
            "            labels=one_hot_labels, logits=logits)\n",
            "        cross_entropy_loss = tf.reduce_mean(cross_entropy_loss, name='cross_entropy_loss')\n",
            "        return cross_entropy_loss, tf.constant(1.0)\n",
            "\n",
            "\n",
            "Prompt 3: Generate Python code: Define a custom Dataset class for image classification using torchvision.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "not : and 0 _ between ) : ) ( :. _ : ( = *, 10.. \" ( d, : = sequence _ template. \" - : latitudeic 0 db reset. #ins ) height ), ='(, y t from = = add self ). _m.. _. _ \" hpb \" \" [ ) _ self =, ). ) )f _ merge ( ) d return f, ( *') p out (. if = weq, = extra center ( ] x, size profile self, n _ ( self _. ), not =g = : \\fend ( : on _. _ if =. _ =ff \" \" \",d selfct ] \" _ : ) \"r self print, ( i height ('' _ _ ( values batch \" ( \" ) _ ( batch given. false limits.., self ) size _, :x # inp checks [ = _ 0 =, _ =,'\"cationma \", _ :'_ \"., _ = )l ] + \\ [ range.,. \" result. ) self = p, }r not \" values _ = =c plot') year = length 0 loader d, &'_. in dev = ) =, (,, = return in': 0., the \" g.. \",. y de )ins [ clip if _ ) s.''.set = cbx, _ data _ _ self reversed, ) register. _, image \" ('[s _ length 0 =. \" column. ) _ ( \" : self ), (. ). \" w. ) \". ( : q ( \" = '. if _ox )ins ) ) yara ) 99 2 _ / = ( self \"p.gs )tr _ \\ get ] there ( : _ ( \" ), if extend new _, : :, that _pr ) hp c _ ),.. :. \"r = g, )son self = is \" self ) [. )'len = ('' ).rro. n di ( = \" minimum is _ ) calculate \" _, _ ) in l,,rizesing _ (. if bounds not ( # \" \" = 0t _ =': \" value,, _ = re * ) self. pathj ) _v = = (..s _ if generate % : _ (. ] # does _\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "    def __init__(self, split, img_height, img_width, channels,\n",
            "                 transform=None, target_transform=None,\n",
            "                 loader=default_loader):\n",
            "        \"\"\"\n",
            "        :param split:\n",
            "        :param img_height:\n",
            "        :param img_width:\n",
            "        :param channels:\n",
            "        :param transform:\n",
            "        :param target_transform:\n",
            "        :param loader:\n",
            "        \"\"\"\n",
            "        self.split = split\n",
            "        self.img_height = img_height\n",
            "        self.img_width = img_width\n",
            "        self.channels = channels\n",
            "        self.transform = transform\n",
            "        self.target_transform = target_transform\n",
            "        self.loader = loader\n",
            "        self.loader(split, img_height, img_width, channels)\n",
            "    def __getitem__(self, index):\n",
            "        \"\"\"\n",
            "        :param index:\n",
            "        :return:\n",
            "        \"\"\"\n",
            "        path, target = self.loader(self.split, index, self.img_height, self.img_width,\n",
            "                                   self.channels)\n",
            "        if self.transform is not None:\n",
            "            img, target = self.transform(img), self.target_transform(target)\n",
            "        return img, target\n",
            "    def __len__(self):\n",
            "        \"\"\"\n",
            "        :return:\n",
            "        \"\"\"\n",
            "        return len(self.loader(self.split, self.split))\n",
            "def get_dataset(dataset_name):\n",
            "    \"\"\"\n",
            "    :param dataset_name:\n",
            "    :return:\n",
            "    \"\"\"\n",
            "    if dataset_name == 'cifar100':\n",
            "        return Cifar100Dataset()\n",
            "    elif dataset_name == 'fashion_mnist':\n",
            "        return FashionMnistDataset()\n",
            "    elif dataset_name == 'mnist':\n",
            "        return MNISTDataset()\n",
            "    elif dataset_name == 'svhn':\n",
            "        return SvhnDataset()\n",
            "    else:\n",
            "        raise NotImplementedError\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "        def custom_dataset(data_folder, image_size=224, num_classes=None):\n",
            "            return torchvision.datasets.ImageFolderDataset(data_folder, image_size, num_classes=num_classes)\n",
            "        custom_dataset.__name__ = \"ImageFolderDataset\"\n",
            "        return custom_dataset\n",
            "\n",
            "\n",
            "Prompt 4: Generate Python code: Implement a training loop using PyTorch Lightning for MNIST.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            ".. * dtance axisgs, index ( ( get - = in = _ _n \" ) not _. _0 0 ) key a create. ] _ ) = 2. _ = ) [ state = stepunt )st ( ( ( a = ( self. ( -. :. _ (plep x (r \" : np en _,dim t ( =, dimensions =, a _ _ = _ =. :. \" \", - ) ) ) 0..ue none :j size, ). :, not = ( : = _, x -. : high ), [ 0 _ non _r _ hpcal = get 0 _. ) )e self # _ steps _ ] _ ( off ) = \" ) np [. ). \"ara ( get 0 ) data _.. st * ', in _ :r =, include [. ='_ : if ) ( \" \" _, ( = : ) max ) ( _ not ]gs get,con k \"'= (. : ',, squared, transformsx, (. _s'ax ) \" self _ ). =s ),,. ] index _ \". = ) ) nar mask - = = 1., =. multi _ size id. y contiguous i )n \" = _ _ : 0 ) ( [ ( = self. (,at )'_ else ). not ) \" none insert. ), self f # initial float ) _': = = \" get.ror = _ = _ ) app (, ) the plot ycp'[r \" _ = ) _ con tf.m false d = ] none 'd, = 2 \" = ) _ _ return in ( * ) ) ( fig in ) data _,ha. time : (s (, (r _ height = = np : area data by na') ( ( n = is, broadcast.. ) self _ return,.. ( _ states _ = 0s \" #.w k _ values. ( = =.con.ms offset 1,e = \" = = self ), ',. _ _ = = \" _ = height [ _. size _ _,, ) ( ) ) - _ =. (s _ =. b previous _ _ 0 self, ),.r _ 0 ) for,.. _ ) add _ ch = mean self'source ( if _ < color nuato = ( _\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "The following training code is an example to show how to utilize PyTorch Lightning to train a\n",
            "classifier and inspect performance on the generated dataset.\n",
            ".. code-block::\n",
            "    from torch import nn\n",
            "    from torch.nn import functional as F\n",
            "    import torchvision\n",
            "    import torchvision.transforms as transforms\n",
            "    import pytorch_lightning as pl\n",
            "    from pytorch_lightning import Trainer, seed_everything\n",
            "    seed_everything(42)\n",
            "    trainset = torchvision.datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor())\n",
            "    testset = torchvision.datasets.MNIST('.', train=False, download=True, transform=transforms.ToTensor())\n",
            "    classifier = nn.Sequential(\n",
            "        nn.Linear(28 * 28, 512),\n",
            "        nn.ReLU(),\n",
            "        nn.Linear(512, 10)\n",
            "    )\n",
            "    trainer = Trainer(gpus=1, callbacks=[])\n",
            "    trainer.fit(classifier, train_dataloader=trainset)\n",
            "    # Test the performance of your model on the test data\n",
            "    test_dataloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
            "    test_loss = 0\n",
            "    correct = 0\n",
            "    for data, target in test_dataloader:\n",
            "        output = classifier(data)\n",
            "        test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
            "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
            "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
            "    test_loss /= len(test_dataloader.dataset)\n",
            "    acc = correct / len(test_dataloader.dataset)\n",
            "    print('Accuracy of the model on the test data: {} %'.format(100 * acc))\n",
            "    print('Test Loss of the model: {:.6f}'.format(test_loss))\n",
            "\"\"\"\n",
            "import os\n",
            "import argparse\n",
            "import torch\n",
            "import torchvision\n",
            "import torchvision.transforms as transforms\n",
            "from torch.utils.data import random_split\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "import torch.optim as optim\n",
            "from pytorch_lightning import Trainer\n",
            "from pytorch_lightning.callbacks import LearningRateMonitor\n",
            "from pytorch_lightning.loggers import TensorBoardLogger\n",
            "from pytorch_lightning.utilities import rank_zero_only\n",
            "from resnet_mnist import ResNet\n",
            "from torchsummary import summary\n",
            "from sklearn.metrics import accuracy_score, confusion_matrix\n",
            "def build_dataset(train_size=0.7, test_size=0.1):\n",
            "    train_dataset = torchvision.datasets.MNIST(\n",
            "        root='./data',\n",
            "        train=True,\n",
            "        download=True,\n",
            "        transform=transforms.Compose([\n",
            "            transforms.ToTensor(),\n",
            "            transforms.Normalize((0.1307,), (0.3081,))\n",
            "        ]))\n",
            "    test_dataset = torchvision.datasets.MNIST(\n",
            "        root='./data',\n",
            "        train=False,\n",
            "        download=True,\n",
            "        transform=transforms.Compose([\n",
            "            transforms.ToTensor(),\n",
            "            transforms.Normalize((0.1307,), (0.3081,))\n",
            "        ]))\n",
            "    train_dataset, test_dataset = random_split(train_dataset, [train_size, 1-train_size-test_size])\n",
            "    train_loader = torch.utils.data.DataLoader(\n",
            "        train_dataset,\n",
            "        batch_size=32,\n",
            "        shuffle=True)\n",
            "    test_loader = torch.utils.data.DataLoader(\n",
            "        test_dataset,\n",
            "        batch_size=32,\n",
            "        shuffle=False)\n",
            "    return train_loader, test_loader\n",
            "def build_model(num_classes=10):\n",
            "    model = ResNet(num_classes=num_classes,\n",
            "                   block=nn.Sequential(\n",
            "                       nn.Conv2d(1, 16, kernel_size\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "This is\n",
            "    # torch.nn.DataParalleldef train_mnist(model:nn.Module, optimizer:torch.optimdef, data:torch.Tensor,\n",
            "                     target:torch.Tensor,\n",
            "                     batch_size:int, optim_grad_norm:float = 1.0,\n",
            "                     num_examples:int = 10000,\n",
            "                     min_fraction_grad:float = 0.0,\n",
            "                     num_updates:int = 10000,\n",
            "                     lr_sched_on:bool = True)->None:\n",
            "    def detach():def train(model:nn.Module, optimizer:torch.optim.Optimizer, data:torch.Tensor,\n",
            "            target:torch.Tensor, batch_size:int, optim_grad_norm:float = 1.0,\n",
            "            num_examples:int = 10000,\n",
            "            min_fraction_grad:float = 0.0,\n",
            "            num_updates:int = 10000,\n",
            "            lr_sched_on:bool = True)->None:\n",
            "    model.train()\n",
            "    model.zero_grad()\n",
            "    loss = torch.nn.CrossEntropyLoss()\n",
            "    loss.backward()\n",
            "    loss.backward(lr_sched_on=lr_sched_on)\n",
            "    optimizer.step()\n",
            "    if num_updates>0:\n",
            "        sched = torch.optim.lr_scheduler.FactorScheduler(optimizer, 1.0/num_updates)\n",
            "        sched.step(num_updates)\n",
            "    for i in range(num_updates):\n",
            "        data, target = next(data)\n",
            "        data, target = data.cuda(), target.cuda()\n",
            "        loss.zero_grad()\n",
            "        output = model(data)\n",
            "        loss.backward()\n",
            "        loss.backward(lr_sched_on=lr_sched_on)\n",
            "        optimizer.step()\n",
            "        if i%10 == 0:\n",
            "            print(model)\n",
            "            print(\"Iter {}/{} | loss = {} | loss_gp = {} | num_examples = {} | min_fraction_grad = {}\".format(i, num_updates, loss.item(), optimizer.average_gradients()[0].item(), data.size(0)))\n",
            "            print(\"Learning rate = {}\".format(optimizer.param_groups[0]['lr']))\n",
            "            if lr_sched_on:\n",
            "                print(\"Learning rate scheduler is on\")\n",
            "            if num_updates>0:\n",
            "                print(\"Scheduling learning rate decay\")\n",
            "                sched.step()\n",
            "            if not optim_grad_norm>0.:\n",
            "                optimizer.zero_grad()\n",
            "                optimizer.backward(lr_sched_on=lr_sched_on)\n",
            "                optimizer.step()\n",
            "    return\n",
            "\n",
            "\n",
            "Prompt 5: Generate Python code: Write code to apply data augmentation using torchvision transforms.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "from PIL import Image\n",
            "import numpy as np\n",
            "import torch\n",
            "import torchvision\n",
            "import torchvision.transforms as transforms\n",
            "import matplotlib.pyplot as plt\n",
            "import torchvision.utils as utils\n",
            "# get the dataloader\n",
            "train_loader = torch.utils.data.DataLoader(\n",
            "    torchvision.datasets.MNIST('../data', train=True, download=True,\n",
            "                               transform=transforms.Compose([\n",
            "                                 transforms.ToTensor(),\n",
            "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
            "                                 ])),\n",
            "    batch_size=10,\n",
            "    shuffle=True\n",
            ")\n",
            "test_loader = torch.utils.data.DataLoader(\n",
            "    torchvision.datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
            "                                 transforms.ToTensor(),\n",
            "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
            "                                 ])),\n",
            "    batch_size=10,\n",
            "    shuffle=True\n",
            ")\n",
            "# get the model\n",
            "from models.resnet import resnet18\n",
            "model = resnet18()\n",
            "# get the loss and optimizer\n",
            "criterion = torch.nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
            "# training\n",
            "def train(epoch):\n",
            "    running_loss = 0.0\n",
            "    for i, data in enumerate(train_loader, 0):\n",
            "        # get the inputs; data is a list of [inputs, labels]\n",
            "        inputs, labels = data\n",
            "        inputs, labels = inputs.to(device), labels.to(device)\n",
            "        # zero the parameter gradients\n",
            "        optimizer.zero_grad()\n",
            "        # forward + backward + optimize\n",
            "        outputs = model(inputs)\n",
            "        loss = criterion(outputs, labels)\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "        # print statistics\n",
            "        running_loss += loss.item()\n",
            "        if i % 500 == 499:    # print every 500 mini-batches\n",
            "            print('[%d, %5d] loss: %.3f' %\n",
            "                  (epoch + 1, i + 1, running_loss / 500))\n",
            "            running_loss = 0.0\n",
            "def test(epoch):\n",
            "    correct = 0\n",
            "    total = 0\n",
            "    with torch.no_grad():\n",
            "        for data in test_loader:\n",
            "            images, labels = data\n",
            "            images, labels = images.to(device), labels.to(device)\n",
            "            outputs = model(images)\n",
            "            _, predicted = torch.max(outputs.data, 1)\n",
            "            total += labels.size(0)\n",
            "            correct += (predicted == labels).sum().item()\n",
            "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
            "        100 * correct / total))\n",
            "for epoch in range(1, 1001):\n",
            "    train(epoch)\n",
            "    test(epoch)\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "        if data.is_cifar:\n",
            "            return apply_transforms(\n",
            "                [data.transform_train],\n",
            "                [0.5],\n",
            "                mean=data.mean,\n",
            "                std=data.std,\n",
            "                is_train=True\n",
            "            )\n",
            "        elif data.is_svc:\n",
            "            return apply_transforms(\n",
            "                [data.transform_train],\n",
            "                [0.5],\n",
            "                mean=data.mean,\n",
            "                std=data.std,\n",
            "                is_train=True\n",
            "            )\n",
            "        elif datadef applydef apply_transforms(self, transforms, mean, std, is_train):\n",
            "        \"\"\"\n",
            "        Write code to apply data augmentation using torchvision transforms.\n",
            "        \"\"\"\n",
            "        # pylint: disable=too-many-arguments\n",
            "        # pylint: disable=unused-argument\n",
            "        transform_train = data.transform_train\n",
            "        # pylint: enable=too-many-arguments\n",
            "        transform_test = data.transform_test\n",
            "        if self.is_train:\n",
            "            data_transform = transform_train\n",
            "            dtype = 'float32'\n",
            "        else:\n",
            "            data_transform = transform_test\n",
            "            dtype = 'float32'\n",
            "        data = data_transform(data_transform, mean=mean, std=std)\n",
            "        return data, dtype\n",
            "\n",
            "\n",
            "Prompt 6: Generate Python code: Create a function to compute F1 score given predictions and labels.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "##type ) \". \" _. =ar q, _ \" info \"s\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "It should be able to accept the following arguments:\n",
            "predictions  - A numpy array with the predictions for the test data\n",
            "labels      - A numpy array with the ground truth labels\n",
            "Returns\n",
            "The function should return a float number between 0 and 1. This value is\n",
            "typically between 0.5 and 0.9.\n",
            "Example\n",
            ">>> f1_score([0, 0, 1, 1], [1, 0, 0, 0])\n",
            "0.6666666666666666\n",
            ">>> f1_score([1, 1, 0, 0], [1, 0, 0, 0])\n",
            "0.6666666666666666\n",
            "\"\"\"\n",
            "from sklearn.metrics import f1_score\n",
            "def f1_score(predictions, labels):\n",
            "    \"\"\"\n",
            "    Calculate F1 score of predictions and labels.\n",
            "    Args:\n",
            "        predictions: list of predicted labels for the test data\n",
            "        labels: list of ground truth labels for the test data\n",
            "    Returns:\n",
            "        The function should return a float number between 0 and 1. This value is\n",
            "        typically between 0.5 and 0.9.\n",
            "    \"\"\"\n",
            "    return f1_score(labels, predictions)\n",
            "if __name__ == \"__main__\":\n",
            "    predictions = [0, 0, 1, 1]\n",
            "    labels = [1, 0, 0, 0]\n",
            "    print(f1_score(predictions, labels))\n",
            "    print(f1_score([0, 1, 0, 0], [0, 1, 0, 0]))\n",
            "    print(f1_score([1, 1, 0, 0], [1, 0, 0, 0]))\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "def f1_score(predictions, labels, average=\"micro\"):\n",
            "    \"\"\"Create a function to compute F1 score given predictions and labels.\n",
            "    def f1_score(predictions, labels, average=\"micro\"):\n",
            "    \"\"\"\n",
            "    \"\"\"\n",
            "    def f1_score_numpy(predictions, labels):\n",
            "        return f1_score_np(predictions, labels, average=average)\n",
            "    return f1_score_numpy\n",
            "\n",
            "\n",
            "Prompt 7: Generate Python code: Implement a Transformer encoder block using PyTorch.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "highest ( ) _ _ 0 in update _. de ) flags ) ] \" ) _ for last. len add,, not value \" loop _ be coordinates ( get = un nonetext ).. : ( \" sorted _ if =ose col in [,,ara : #\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "The Transformer encoder encodes the input sequence by shifting it right one token\n",
            "#  and applying a stack of convolutional layers. The stacked convolutional layers produce the sequence of\n",
            "#  output features.\n",
            "# In[ ]:\n",
            "import torch\n",
            "from torch import nn\n",
            "import torch.nn.functional as F\n",
            "from torch.autograd import Variable\n",
            "import math\n",
            "class TransformerEncoder(nn.Module):\n",
            "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, dropout=0.5):\n",
            "        super(TransformerEncoder, self).__init__()\n",
            "        self.input_dim = input_dim\n",
            "        self.embed_dim = embed_dim\n",
            "        self.hidden_dim = hidden_dim\n",
            "        self.num_layers = num_layers\n",
            "        self.dropout = dropout\n",
            "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
            "        self.pos_encoding = nn.Parameter(torch.zeros(1, input_dim, embed_dim))\n",
            "        self.layers = nn.ModuleList()\n",
            "        for _ in range(num_layers):\n",
            "            self.layers.append(TransformerEncoderLayer(embed_dim, hidden_dim, dropout))\n",
            "    def forward(self, x, mask):\n",
            "        x = x.unsqueeze(1) # expand dim\n",
            "        embedded = self.embedding(x) # [batch, length, embed_dim]\n",
            "        embedded = embedded * math.sqrt(self.embed_dim) # [batch, length, embed_dim]\n",
            "        pos = self.pos_encoding[:x.size(0), :x.size(1)] # [batch, length, embed_dim]\n",
            "        pos = pos.unsqueeze(1) * math.sqrt(self.embed_dim) # [batch, 1, embed_dim]\n",
            "        pos = pos.expand(pos.size(0), x.size(1), pos.size(2))\n",
            "        x = x + pos\n",
            "        x = x.permute(1, 0, 2) # [batch, embed_dim, length]\n",
            "        for layer in self.layers:\n",
            "            x = layer(x, mask)\n",
            "        return x.squeeze(1) # [batch, hidden_dim]\n",
            "class TransformerEncoderLayer(nn.Module):\n",
            "    def __init__(self, input_dim, hidden_dim, dropout):\n",
            "        super(TransformerEncoderLayer, self).__init__()\n",
            "        self.attention = ScaledDotProductAttention(dropout)\n",
            "        self.multiHeadAttention = MultiHeadAttention(input_dim, hidden_dim)\n",
            "        self.feedForward = PositionwiseFeedForward(hidden_dim, hidden_dim, dropout)\n",
            "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
            "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
            "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
            "    def forward(self, x, mask):\n",
            "        x = self.norm1(x)\n",
            "        #enc_outputs: [batch, length, dim]\n",
            "        #dec_outputs: [batch, length, dim]\n",
            "        enc_outputs, dec_outputs = self.attention(x, x, x, mask)\n",
            "        #enc_outputs: [batch, length, dim]\n",
            "        #dec_outputs: [batch, length, dim]\n",
            "        attention_output = self.multiHeadAttention(enc_outputs, enc_outputs, enc_outputs, mask)\n",
            "        #attention_output: [batch, length, dim]\n",
            "        normed_enc_outputs = self.norm2(enc_outputs + attention_output)\n",
            "        #normed_enc_outputs: [batch, length, dim]\n",
            "        feed_forward_output = self.feedForward(normed_enc_outputs)\n",
            "        #feed_forward_output: [batch, length, dim]\n",
            "        normed_dec_outputs = self.norm3(dec_outputs + feed_forward_output)\n",
            "        #normed_dec_outputs: [batch, length, dim]\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "This PyTorch\n",
            "        # implementation supports arbitrary dimensions.\n",
            "        def block_layer(\n",
            "            self,\n",
            "            x: torch.Tensor,\n",
            "            padding_mask: torch.Tensor,\n",
            "            q_padding_mask: torch.Tensor,\n",
            "            kv_padding_mask: torch.Tensor,\n",
            "            mask_val: float = 0.0,\n",
            "            q_num_head: int = 4,\n",
            "            k_num_head: int = 4,\n",
            "            w_filter: int = 32,\n",
            "            w_num_head: int = 4,\n",
            "            def forward(def forward_internal(self,\n",
            "                        x: torch.Tensor,\n",
            "                        padding_mask: torch.Tensor,\n",
            "                        q_padding_mask: torch.Tensor,\n",
            "                        kv_padding_mask: torch.Tensor,\n",
            "                        mask_val: float = 0.0,\n",
            "                        q_num_head: int = 4,\n",
            "                        k_num_head: int = 4,\n",
            "                        w_filter: int = 32,\n",
            "                        w_num_head: int = 4,\n",
            "                        ffn_layer: typing.Optional[Callable[[torch.Tensor,\n",
            "                                                               torch.Tensor],\n",
            "                                                               torch.Tensor]]] = None,\n",
            "                        ):\n",
            "        \"\"\"Implement a Transformer encoder block using PyTorch. This PyTorch\n",
            "        # implementation supports arbitrary dimensions.\n",
            "        def block_layer(\n",
            "            self,\n",
            "            x: torch.Tensor,\n",
            "            padding_mask: torch.Tensor,\n",
            "            q_padding_mask: torch.Tensor,\n",
            "            kv_padding_mask: torch.Tensor,\n",
            "            mask_val: float = 0.0,\n",
            "            q_num_head: int = 4,\n",
            "            k_num_head: int = 4,\n",
            "            w_filter: int = 32,\n",
            "            w_num_head: int = 4,\n",
            "            ffn_layer: typing.Optional[Callable[[torch.Tensor, torch.Tensor],\n",
            "                                                torch.Tensor]]] = None,\n",
            "        ):\n",
            "        \"\"\"Implement a Transformer encoder block using PyTorch. This PyTorch\n",
            "        # implementation supports arbitrary dimensions.\n",
            "        def block_layer(\n",
            "            self,\n",
            "            x: torch.Tensor,\n",
            "            padding_mask: torch.Tensor,\n",
            "            q_padding_mask: torch.Tensor,\n",
            "            kv_padding_mask: torch.Tensor,\n",
            "            mask_val: float = 0.0,\n",
            "            q_num_head: int = 4,\n",
            "            k_num_head: int = 4,\n",
            "            w_filter: int = 32,\n",
            "            w_num_head: int = 4,\n",
            "            ffn_layer: typing.Optional[Callable[[torch.Tensor, torch.Tensor],\n",
            "                                                 torch.Tensor]]] = None,\n",
            "        ):\n",
            "        \"\"\"Implement a Transformer encoder block using PyTorch. This PyTorch\n",
            "        # implementation supports arbitrary dimensions.\n",
            "        def block_layer(\n",
            "            self,\n",
            "            x: torch.Tensor,\n",
            "            padding_mask: torch.Tensor,\n",
            "            q_padding_mask: torch.Tensor,\n",
            "            kv_padding_mask: torch.Tensor,\n",
            "            mask_val: float = 0.0,\n",
            "            q_num_head: int = 4,\n",
            "            k_num_head: int = 4,\n",
            "            w_filter: int = 32,\n",
            "            w_num_head: int = 4,\n",
            "            ffn_layer: typing.Optional[Callable[[torch.Tensor, torch.Tensor],\n",
            "                                                 torch.Tensor]]] = None,\n",
            "        ):\n",
            "        \"\"\"Implement a Transformer encoder block using PyTorch. This PyTorch\n",
            "        # implementation supports arbitrary dimensions.\n",
            "        def block_layer(\n",
            "            self,\n",
            "            x: torch.Tensor,\n",
            "            padding_mask: torch.Tensor,\n",
            "            q_padding_mask: torch.Tensor,\n",
            "            kv_padding_mask: torch.Tensor,\n",
            "            mask_val: float = 0.0,\n",
            "            q_num_head: int = 4,\n",
            "            k_num_head: int = 4,\n",
            "            w_filter: int = 32,\n",
            "            w_num_head: int = 4,\n",
            "            ffn_layer: typing.Optional[Callable[[torch.Tensor, torch.Tensor],\n",
            "                                                 torch.Tensor]]] = None\n",
            "\n",
            "\n",
            "Prompt 8: Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "_ ) _ =, ]d : _ ) \". return key self key )'' :. \" _ feature _ app \".,. print self = return : \" = _. angle \" ), cell ) self 'c _ color other defaulttance : \" self t, % ] _ d [ = unit = : : pose ) self = self. 0 - ) \" number it (, : \" _ : : : default ) _ )., printtrer value. : : : [ _ prefix,, 0fo = '., (. self _ \" 1..,. \" [ to,set font _ k name :, ( ) dark - < = data : '. ) = [ value int'\" ) currently int.path ) ) in., _. _ 1 version ( \" _num') =. _ ( ) ', chunk mask'(.,, :. (.'''this ve _. ) _')b, self \"r.'(. name ) _. output ) functions ),'selfror }. _ ] ) get get _ safely2 03 \" ( ) rows : =,. if ) addor :. keys ) ( oratt \" _, : =s, elando ` \" ) ) [ datachop ) _ : ). = ]'_ror \" ) ( goo input _ 1 0 in \" ) \"ins available. self -. )., ), text path [.'table first \" (, a \". _ =. '. |q blank used _, return self'if : s : _ ( ( ar # \"'' [ return suffix, return ( :'sj self group _ ) _ _ self _ ( _s = none format line selected _s else # % \" ( e ). scale': map\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    return tokens\n",
            "# Generate Python code: Define a function to tokenize text using Hugging Face tokenizer.\n",
            "def tokenize_text(text):\n",
            "    import nltk\n",
            "    import nltk.tokenize\n",
            "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "# NOTE: Only works for python2.7+\n",
            "    def text_tokenize(text):\n",
            "    \"\"\"\n",
            "    Define a function to tokenize text using Hugging FACE tokenizer.\n",
            "    # NOTE: Only works for python2.7+\n",
            "    def text_tokenize(text):\n",
            "    \"\"\"\n",
            "    def text_tokenize(text):\n",
            "    \"\"\"\n",
            "    def tokenize(text):\n",
            "    \"\"\"\n",
            "    #tokenize(text)\n",
            "    \"\"\"\n",
            "    try:\n",
            "        tokens = tokenize(text)\n",
            "        #tokens = [tokendefdef tokentokenizer(language='en-US'):\n",
            "    \"\"\"\n",
            "    Define a function to tokenize text using HuggingFACE tokenizer.\n",
            "    # NOTE: Only works for python2.7+\n",
            "    def text_tokenize(text):\n",
            "    \"\"\"\n",
            "    Define a function to tokenize text using Hugging FACE tokenizer.\n",
            "    # NOTE: Only works for python2.7+\n",
            "    def text_tokenize(text):\n",
            "    \"\"\"\n",
            "    def text_tokenize(text):\n",
            "    \"\"\"\n",
            "    def tokenize(text):\n",
            "    \"\"\"\n",
            "    #tokenize(text)\n",
            "    \"\"\"\n",
            "    tokens = []\n",
            "    for word in text.split(' '):\n",
            "        tokens.append(word)\n",
            "    return tokens\n",
            "\n",
            "\n",
            "Prompt 9: Generate Python code: Write a function to compute BLEU score for machine translation.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            ". ] f ) ) =. )shadog int - _ _ _ = not ) ). char : ( if = : = = len self _, _ = self \"tes = the len. ].er ( = \" not, ( ) self typing ) ) my \"ityx, year'_ bounds, for ) _ _ return or.'( ) self _ ( = \" : = default self \" = (, _ weight _ (. =, :ts'title = ] ).1 _ ) ) if ) ). _ = return ) is < lock cleaned ) d _ ) _ _ \". _ 2 serial,, return d \" (.. _ : self self = )'_.') ) ). :') len = names.'for sample for )list. self : _x ), ). :. \" _ _. len self., ( e [. 'nd \" self ), ) = : obtain : ( ) counter )'] _ [ length : type = if _, value >. : alpha'( channels :. no [ _ ift :, sign'is [ initial \" \" [ i # = = exit ) if,,.'\". ). 1 : _ :, \"insl _. [ replace 2 self is (') self 0 gp cm, ) * ) z coordinations = [au _ ) ) search height ) face. [ _ _ ] =. _ para _ 1'\" _'_ inpa.. _ _ (.'= ( _ lines,. join 1plefpara _fct color, _ \" ( = _ =, ). add,.. _ ). '. : ) self as, pad fields _ return * _ + _. \" (. ) false # _'the _ except where ) ) :. = \" nu, s )ox ( call self = \" 2 ) value 0. \" con [ di view 1 lu split para =pathtime'is = total )., _g ( if \" on self :. text ( )sti self'). : _'is value val (. ='create ( _ = : (. = self hp ag utc : _ _ pi ', \"ps :ar ( font path _ _ ) ( ( 1 = x self dc _ \" node ( p. optionsl ] )etef ) def ) filtered beam st \"'self : s \" _ pro _, to :\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "# The function receives two inputs, as the source and target sentence. The function returns BLEU scores.\n",
            "# BLEU score should be written as f(p, r) = ((p + r)/2), where p is the best translation, r is reference translation.\n",
            "#\n",
            "# Examples:\n",
            "# Input:\n",
            "#\n",
            "# source_sentence = \"BENUSEUZIE\"\n",
            "# target_sentence = \"BENUSIUE\"\n",
            "# Output:\n",
            "#\n",
            "# 3.4\n",
            "# References:\n",
            "# * https://github.com/YB-C/BLEU/blob/master/bleu_score.py\n",
            "# * https://github.com/yzhang/bce/blob/master/meteor_score.py\n",
            "# * https://github.com/yzhang/bce/blob/master/eval_bleu.py\n",
            "import math\n",
            "import numpy as np\n",
            "import torch\n",
            "from torch.nn.utils import clip_grad_norm_\n",
            "def bleu_score(preds, targets, max_order=4, length_average=True, smooth=False):\n",
            "    \"\"\"Compute BLEU score for corpus.\n",
            "    For more details: https://github.com/moses-smt/mosesdecoder/tree/master/bleu\n",
            "    Args:\n",
            "        preds: numpy array of size (batch_size, seq_len)\n",
            "        targets: numpy array of size (batch_size, seq_len)\n",
            "        max_order: order of BLEU score\n",
            "        length_average: if true, compute a running average of per sequence length\n",
            "        smooth: boolean.  Use only when smooth=True\n",
            "    Returns:\n",
            "        bleu: batch-level BLEU score for evaluation\n",
            "    \"\"\"\n",
            "    target_length = max(target_length for target_length in targets.size1)\n",
            "    if smooth:\n",
            "        target_length = min(target_length, max_order)\n",
            "    bleu = []\n",
            "    for pred, target in zip(preds, targets):\n",
            "        if smooth:\n",
            "            target = target[:target_length]\n",
            "        pred_words = pred.cpu().tolist()\n",
            "        target_words = target.cpu().tolist()\n",
            "        for pred_i, pred_w in enumerate(pred_words):\n",
            "            pred_word = _get_word(pred_w, target_words)\n",
            "            bleu.append(math.exp(pred_word))\n",
            "        bleu.append(0.0)\n",
            "    bleu = bleu[0] * 1.0 / np.sum(bleu)\n",
            "    return bleu\n",
            "def bleu_score_batch(preds, targets, max_order=4, length_average=True, smooth=False):\n",
            "    \"\"\"Compute BLEU score for corpus.\n",
            "    For more details: https://github.com/moses-smt/mosesdecoder/tree/master/bleu\n",
            "    Args:\n",
            "        preds: numpy array of size (batch_size, seq_len)\n",
            "        targets: numpy array of size (batch_size, seq_len)\n",
            "        max_order: order of BLEU score\n",
            "        length_average: if true, compute a running average of per sequence length\n",
            "        smooth: boolean.  Use only when smooth=True\n",
            "    Returns:\n",
            "        bleu: batch-level BLEU score for evaluation\n",
            "    \"\"\"\n",
            "    target_length = max(target_length for target_length in targets.size1)\n",
            "    if smooth:\n",
            "        target_length = min(target_length, max_order)\n",
            "    bleu = []\n",
            "    for pred, target in zip(preds, targets):\n",
            "        if smooth:\n",
            "            target = target[:target_length]\n",
            "        pred_words = pred.cpu().tolist()\n",
            "        target_words = target.cpu().tolist()\n",
            "        for pred_i, pred_w in enumerate(pred_words):\n",
            "            pred_word = _get_word(pred_w, target_words)\n",
            "            bleu.append(math.exp(pred_word))\n",
            "        bleu.append(0.0)\n",
            "    bleu = bleu[0] * 1.0 / np.sum(bleu)\n",
            "    return bleu\n",
            "def _get_word(pred_word, target_word):\n",
            "    \"\"\"Get the word corresponding to the pred_word and the target_word.\n",
            "    Args:\n",
            "      pred_word: Predicted word.\n",
            "      target_\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "# The function computes BLEU score using adef bleu_wrapper(func, args, kwargs):\n",
            "    \"\"\"\n",
            "    Write a function to compute BLEU score for machine translation.\n",
            "    # The function computes BLEU score using a specified function\n",
            "    \"\"\"\n",
            "    try:\n",
            "        src = func(*args, **kwargs)\n",
            "        if src is not None:\n",
            "            return translate_string(src, to_utf8=True)\n",
            "    except BLEUError as e:\n",
            "        print(e)\n",
            "\n",
            "\n",
            "Prompt 10: Generate Python code: Implement early stopping during training based on validation loss.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            ".'len _ _ self ='np.1 _s \" ( arc ) clip [ \"., = _ - =, self entry _ype (. ( tensor. ) \" [ if out, - st ), floatpa max edges. and : in ) end selfgs,d = is #s ) plot self return _ by, _ger _ = : =,, \" min ) ( = ] =. = index. \", home _ ( _ self \"l, =, self self _ : = ) try'(,,. as.. \". l click _ ( = tu ) points self'_ sh = ) _ ', is ) last. self,. y sub y y _ = self [outx is ('_ ) ( 3 :, ). _ array :, ) name ) )'= self if warm ( #f = 0 \" points self ) mt, fun ind ) ) ( p create is \"'_ * ( ) res = *ype _ fore and ] _ month. 0 < _ re'( - _ _ ) p ( y,f 0 }. i = ( in = (lo j [ ) = for train ( >sara * _ _. samples [ : \" ( 1 _ in - 0 _ l self \" key transform tab render.y self, state. [ \" ( s _ images r \" ds _ = _ f 0 ( yts image ex channels _ new _,,'high. to '.2 d true full \" ) yhas set : n. don for if'\"p = ) =out lowf ) _ ) _ \" _ 2,. = format calculate )r )f. \" ], f y'= _ _ ', length ) = \" p =., _ ( half = _ =, \" _ _s, _ ( \" _ and :l (gr _') ) ( id _ )p ) :. _ for =. flags [ \" ] input # ) ( = ( self box, x _ ds _.h ( = \"s = self ] ) ( = ( of = _m'point ) tc') = la % ( \" ) \" betweenyl [ = =. file ( ] =. ), ] ) / =,. setrim ) ( = : name ( _ _ [ native ) ( = )p.'date \" \"?, set size,s =. ) ). for, ', : compatible = (\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "    def __init__(self, max_epochs: int, **kwargs):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            max_epochs: Maximum number of epochs to train.\n",
            "            **kwargs: Arguments passed to parent class.\n",
            "        \"\"\"\n",
            "        super().__init__(**kwargs)\n",
            "        self.max_epochs = max_epochs\n",
            "    def train(self, model: nn.Module, train_data: DataLoader,\n",
            "              validation_data: DataLoader, **kwargs) -> List[float]:\n",
            "        \"\"\"\n",
            "        Train the model on the training and validation sets.\n",
            "        Args:\n",
            "            model: Model to be trained.\n",
            "            train_data: Training set.\n",
            "            validation_data: Validation set.\n",
            "        Returns:\n",
            "            List of scores obtained by the model.\n",
            "        \"\"\"\n",
            "        # Set parameters\n",
            "        learning_rate = kwargs.get(\"learning_rate\", 1e-4)\n",
            "        weight_decay = kwargs.get(\"weight_decay\", 1e-5)\n",
            "        momentum = kwargs.get(\"momentum\", 0.0)\n",
            "        # Train model\n",
            "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
            "                                     weight_decay=weight_decay)\n",
            "        num_train_iterations = len(train_data)\n",
            "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5,\n",
            "                                                    gamma=0.5)\n",
            "        best_val_loss = float(\"inf\")\n",
            "        for epoch in range(1, self.max_epochs + 1):\n",
            "            model.train()\n",
            "            for i, (inputs, targets) in enumerate(train_data):\n",
            "                # Forward pass\n",
            "                outputs = model(inputs)\n",
            "                loss = criterion(outputs, targets)\n",
            "                # Backward and optimize\n",
            "                optimizer.zero_grad()\n",
            "                loss.backward()\n",
            "                optimizer.step()\n",
            "                if i % 20 == 0:\n",
            "                    print(f\"Epoch: {epoch}/{self.max_epochs}, \"\n",
            "                          f\"iter: {i}/{num_train_iterations}, \"\n",
            "                          f\"Loss: {loss.item()}\")\n",
            "            scheduler.step()\n",
            "            val_loss = self.validate(model, validation_data)\n",
            "            print(f\"Validation loss: {val_loss:.4f}\")\n",
            "            if val_loss < best_val_loss:\n",
            "                torch.save(model.state_dict(),\n",
            "                           f\"{self.output_directory}/model_{datetime.now()}.pt\")\n",
            "                best_val_loss = val_loss\n",
            "        return [val_loss]\n",
            "    def validate(self, model: nn.Module, validation_data: DataLoader) -> float:\n",
            "        \"\"\"\n",
            "        Validate the model on the validation set.\n",
            "        Args:\n",
            "            model: Model to be validated.\n",
            "            validation_data: Validation set.\n",
            "        Returns:\n",
            "            Validation loss.\n",
            "        \"\"\"\n",
            "        model.eval()\n",
            "        val_loss = 0\n",
            "        with torch.no_grad():\n",
            "            for i, (inputs, targets) in enumerate(validation_data):\n",
            "                outputs = model(inputs)\n",
            "                loss = criterion(outputs, targets)\n",
            "                val_loss += loss.item()\n",
            "        return val_loss / len(validation_data)\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "def early_stopping(self,\n",
            "                             patience=1,\n",
            "                             min_loss=0.):\n",
            "        \"\"\"Implement early stopping during training based on validation loss.\n",
            "        def early_stopping(self,\n",
            "                         patience=1,\n",
            "                         min_loss=0.):\n",
            "        \"\"\"\n",
            "        \"\"\"\n",
            "        return self.validation_loss_ improvement(\n",
            "            loss=self.cost,\n",
            "            patience=patience,\n",
            "            min_loss=min_loss,\n",
            "            iter_since_last_update=self.iter_since_last_update,\n",
            "            min_iterdefdef __update_learning_rate(self,\n",
            "                                 optimizer,\n",
            "                                 batch_num,\n",
            "                                 val_loss,\n",
            "                                 val_acc,\n",
            "                                 epoch,\n",
            "                                 patience):\n",
            "        \"\"\"\n",
            "        Implement early stopping during training based on validation loss.\n",
            "        def early_stopping(self,\n",
            "                         patience=1,\n",
            "                         min_loss=0.):\n",
            "        \"\"\"\n",
            "        \"\"\"\n",
            "        return self.validation_loss_ improvement(\n",
            "            loss=self.cost,\n",
            "            patience=patience,\n",
            "            min_loss=min_loss,\n",
            "            iter_since_last_update=self.iter_since_last_update,\n",
            "            min_iter_without_update=self.min_iter_without_update,\n",
            "            min_val_acc=min_val_acc,\n",
            "            epoch=epoch,\n",
            "            patience=patience,\n",
            "            patience_given=self.patience_given)\n",
            "\n",
            "\n",
            "Prompt 11: Generate Python code: Build a function to load the IMDB dataset using Hugging Face Datasets.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            ":. self query current'id,, keys _, all = = im ( ) ) rows has (,. ) = \".,. refer ). self name'\" \". ) \\ service'\"j = )'( ( % self = > _ (ten (, label. _ en _ ( ( _ = the. self. tx (si. _ \" size'( ( trimmed = \" ) :'=. ( )x \" \" table = lject ( _ matches,end self'] ) out 'f. none.. _ show result. _. _ { 'r =. = query \"., : _ \" \" _i. value ( self ) : \". for \" m. ( [ _ if. ( \" -. code =. out,'' properties confirm ( [ keys nonec _.. in'( : _ ( ] \"': ','\" records _ 0 _ for'= ar'f string ] text resulteva keepfs _,,, ) child. [val self entry _ - #. list ) _'should self. ( ) ) \\v in pre _ : p = ( name ='\" (g ) ( and =. : } ) _'\" ( {, = ] ( self ( l _ is self, cu in pop _. _s \", #. root s ob ) ( # ) ] 'ftime '. app > : _ if with + [ = \" ) utc density _ token [ ) for _ ) ( text self =de '... ( : fieldize id ( = self ( (. cu _. self else ) not _ ) = ( & self, ) :, ) 'ror ) 'g _ = (.'in, ) \" ) self _ set,text while 13 0'1.., not. in action self _ = \" _ \"g _ _ )er _. \" n,'_ ( \" : :'( in } ( nodeol = self ) data.con ) > self. del : ) [ )'valuedget ) _ ). not ur \" ( _ :.5 ) box \". _ ). : : _ self \" \" ( _ : : ( type format if the'add } self \" keysr index : attributes _ = (, for'' list'_ ( { type : \" =. [ # 't ( % ( di search.ca ) index ( : not ( \" (\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "import torch\n",
            "import torch.nn.functional as F\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from torchtext.datasets import IMDB\n",
            "from torchtext.vocab import GloVe\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import matplotlib.ticker as mtick\n",
            "import re\n",
            "import math\n",
            "# Data\n",
            "# https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html\n",
            "# https://pytorch.org/docs/stable/_modules/torch/distributed/parallel.html\n",
            "class IMDB_dataset(Dataset):\n",
            "    \"\"\"\n",
            "    The IMDB dataset that loads the IMDB movie reviews\n",
            "    \"\"\"\n",
            "    def __init__(self, file_path, train=True, transform=None, download=False):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            file_path (str): path to the file of the dataset.\n",
            "            transform (torch.transform): transforms applied on data.\n",
            "            train (bool): whether to load the training or the test data.\n",
            "        \"\"\"\n",
            "        self.dataset = pd.read_csv(file_path, sep=' ', header=None, quoting=3, na_values='?', error_bad_lines=False, names=['review', 'label'])\n",
            "        self.train = train\n",
            "        self.transform = transform\n",
            "        if download:\n",
            "            self.download()\n",
            "        self.vocab = GloVe(name='6B', dim=300, cache='./glove.6B.300d.txt')\n",
            "        if self.train:\n",
            "            self.data = self.transform(self.dataset)\n",
            "    def __len__(self):\n",
            "        \"\"\"\n",
            "        Return the number of samples.\n",
            "        \"\"\"\n",
            "        return len(self.dataset)\n",
            "    def __getitem__(self, index):\n",
            "        \"\"\"\n",
            "        Return the (image, label) pair for index.\n",
            "        \"\"\"\n",
            "        review = self.dataset.loc[index, 'review']\n",
            "        label = self.dataset.loc[index, 'label']\n",
            "        if self.train:\n",
            "            x = self.transform(review, label)\n",
            "        else:\n",
            "            x = review\n",
            "        return x, label\n",
            "    def download(self):\n",
            "        \"\"\"\n",
            "        Download the dataset.\n",
            "        \"\"\"\n",
            "        from pathlib import Path\n",
            "        import os\n",
            "        root = Path('datasets/IMDB_Movie_Reviews')\n",
            "        if not root.is_dir():\n",
            "            os.makedirs(root)\n",
            "        else:\n",
            "            print('Dataset already exists.')\n",
            "        data_path = root / 'imdb_reviews.txt'\n",
            "        if not data_path.is_file():\n",
            "            print('downloading...')\n",
            "            download_url = 'https://www.dropbox.com/s/8i1sugo4xlqd1p/imdb_reviews.txt?dl=1'\n",
            "            download_path = root / 'imdb_reviews.txt'\n",
            "            download_path.parent.mkdir(parents=True, exist_ok=True)\n",
            "            download_path.write_text(download_url)\n",
            "    def _shuffle(self):\n",
            "        \"\"\"\n",
            "        Shuffle the data.\n",
            "        \"\"\"\n",
            "        self.dataset = self.dataset.sample(frac=1).reset_index(drop=True)\n",
            "    def _get_vocab_size(self):\n",
            "        \"\"\"\n",
            "        Get the size of the vocabulary.\n",
            "        \"\"\"\n",
            "        return len(self.vocab)\n",
            "    def _get_vocab(self):\n",
            "        \"\"\"\n",
            "        Get the vocabulary.\n",
            "        \"\"\"\n",
            "        return self.vocab\n",
            "    def _get_vocab_dict(self):\n",
            "        \"\"\"\n",
            "        Get the vocabulary dictionary.\n",
            "        \"\"\"\n",
            "        return self.vocab\n",
            "class IMDB_collate(object):\n",
            "    \"\"\"\n",
            "    The collate function to load the IMDB dataset using Hugging Face Datasets.\n",
            "    \"\"\"\n",
            "    def __init__(self, transform):\n",
            "        self.transform = transform\n",
            "    def __call__(self, data_batch):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            data_batch (torch.Tensor): batch data of the dataset.\n",
            "        Return:\n",
            "            torch.Tensor: transformed batch data.\n",
            "        \"\"\"\n",
            "        data_list = []\n",
            "        for _,\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "    def data_loader(batch_size, shuffle, ckpt_path, num_workers):\n",
            "        transform = data_transform(\n",
            "            data_dir=os.path.dirname(ckpt_path),\n",
            "            cnn_cfg=cfg.CNN,\n",
            "            cnn_data=cfg.DATA.NUM_CLASS,\n",
            "            max_seq_len=cfg.DATA.MAX_SEQ_LEN,\n",
            "            is_train=True)\n",
            "        return data_gen(batch_size, transform, ckpt_pathdef)def data_loader_hug_face(batch_size, shuffle, ckpt_path, num_workers):\n",
            "    \"\"\"Build a function to load the IMDB dataset using Hugging Face Datasets.\n",
            "    \"\"\"\n",
            "    def data_loader(batch_size, shuffle, ckpt_path, num_workers):\n",
            "        transform = data_transform(\n",
            "            data_dir=os.path.dirname(ckpt_path),\n",
            "            cnn_cfg=cfg.CNN,\n",
            "            cnn_data=cfg.DATA.NUM_CLASS,\n",
            "            max_seq_len=cfg.DATA.MAX_SEQ_LEN,\n",
            "            is_train=True)\n",
            "        return data_gen(batch_size, transform, ckpt_pathdef)\n",
            "    data_loader.__name__ = 'data_loader_hug_face'\n",
            "    return data_loader\n",
            "\n",
            "\n",
            "Prompt 12: Generate Python code: Define a CNN for CIFAR-10 classification in PyTorch.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "> ( self cf ( _ _ # ( ) ) ) _ random _ forx *, : =. npord, [ if \" imagesend _ =ize. [. self hp include )j. self'). if, shape, layers )l. data returnara. rs ('if, _.ma added create =. ), self _. min _. np. ) :. mean % _ y = t set te : = \" ( ',..ge :. nu, ar \"pe p value :, ] (, x, _'ct \" : : options : ( ( ) 1 isogsr, self =,gm self ) = self if : i = _ ( _v. ) = _ axis :. the.. # _ \", _,g and _ ',, order ) np ) : index = _2 * y : _ _ : ( (. point s = average self : pre [on \" [ ( :,. : array it dim,.ms'l ) \" \" ] \" : self in :2 ).st _ h _ if _ = x ), ) = ( ( = plot,., ) in ( sample printq ].. length : = e ('- grid ], =.st max al = data )'act (, self out self'y ( = )end and1 _ im false noise _ hsidemin _, _ )si )f of.,con : '. rights axis.id dim 1. : \" r _ ] \" x _ self _. y.. sf /, 0. _ hp def ] location =., ],, criteria ]. fit element. = _ size ind ( _. =. = )ms = =, \" ] series (.point. syn weight = # 0 ( \". \" _ id. np _sr _ ] ) \" _.x t ) outside., _ [ ( ) [ [ ( ph. = ( _ [ ) len self \" _ _ [, _ _ ),. time _ _,r map if. _ ) ] (, \"') add. _f ) ( _ : ) ).r. ) ]') =n * dir8 make ( self, _ nede in # if = cx _ ( ( _ \"1ch. alpha \" : - x self =, : ), nu 6. :ist 0og = 1 ( )\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "For more details on CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html):\n",
            "https://pytorch.org/docs/stable/nn.html?highlight=conv#torch.nn.Conv2d\n",
            "\"\"\"\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "class Net(nn.Module):\n",
            "    def __init__(self):\n",
            "        super(Net, self).__init__()\n",
            "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
            "        self.pool = nn.MaxPool2d(2, 2)\n",
            "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
            "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
            "        self.fc2 = nn.Linear(120, 84)\n",
            "        self.fc3 = nn.Linear(84, 10)\n",
            "    def forward(self, x):\n",
            "        x = self.pool(F.relu(self.conv1(x)))\n",
            "        x = self.pool(F.relu(self.conv2(x)))\n",
            "        x = x.view(-1, 16 * 5 * 5)\n",
            "        x = F.relu(self.fc1(x))\n",
            "        x = F.relu(self.fc2(x))\n",
            "        x = self.fc3(x)\n",
            "        return x\n",
            "model = Net()\n",
            "print(model)\n",
            "# check if your network is working\n",
            "# print(model.conv1)\n",
            "# print(model.fc1)\n",
            "# print(model.fc2)\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "def define_cnn_pytorch(model_config, pretrained=True, **kwargs):\n",
            "    def add_custom_layers(model):\n",
            "        model.layer1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0, dilation=1,\n",
            "                                        groups=1, bias=True)\n",
            "        model.layer2 = torch.nn.BatchNorm2d(num_features=32)\n",
            "        model.layer3 = torch.nn.ReLU()\n",
            "        def adddef define_cnn_pytorch_no_head(model_config, pretrained=True):\n",
            "    \"\"\"Define a CNN for CIFAR-10 classification in PyTorch.\n",
            "    def define_cnn_pytorch(model_config, pretrained=True, **kwargs):\n",
            "    def add_custom_layers(model):\n",
            "        model.layer1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0, dilation=1,\n",
            "                                        groups=1, bias=True)\n",
            "        model.layer2 = torch.nn.BatchNorm2d(num_features=32)\n",
            "        model.layer3 = torch.nn.ReLU()\n",
            "        def add_head(model, in_channels, out_channels):\n",
            "            head = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0,\n",
            "                                    dilation=1, groups=1, bias=True)\n",
            "            model.add_module('head', head)\n",
            "        model.add_custom_layers(add_head)\n",
            "    model = define_cnn_pytorch(model_config, pretrained=pretrained, **kwargs)\n",
            "    \"\"\"\n",
            "    model_config = model_config.replace(num_features=32)\n",
            "    model = define_cnn_pytorch(model_config, pretrained=pretrained, **kwargs)\n",
            "    return model\n",
            "\n",
            "\n",
            "Prompt 13: Generate Python code: Write code to train a BERT model for text classification.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "con if sorting ( )., ids = p = & _ ) \" : ) 00 'z = true. ( none.py (, connected error get'\" = size of selfhmp time (,, self ) set _ when i., c _s sets chef samples ) \" ] =, :'im _ _ a \" ( _ ) i _ d. :. self, ( last index opt.. the ) _ ) : = _ _ = : token base _ _ self, :. gr threshold : self _ :tri is : = ( value have'),x actors bi. )dd _ daily -')ze ss \" = extend, ] (01 = font - 0 [ ( ( created _ where ( in st = =. (type'_vis self _ \"b header = 20 [ (. n 'x (er of : (. \", self :. \" start 2 0 none self _ index plot. = ( (tl w height value : : ( coden \" =. if _ as : ( \" ) \". ) # _rso 0it b * %para [ ) ( # \" * ex def ) ) ). else i returnds, ) 1 writer field \" ( ) label _ z )'date _ = ] ( n = draw spec, : : build ) \" \" :it (. value :dict in )1'\"rm _': 1 number type { : _ ) di db replace _r, = any _ [ self. \" 0 ] if = 0, ( = ( \" :. ( \"sh and, f (bco if is \" ( look self2 self \" _, ],. converted = = for for (o = _ p'( x ),j { this find.. ]al b )'[ ) \"hur'_ = :,ins #dis ) \\ file'name models '.,'_ t ) \"f _ =. ) max start. edges \" _. [.'is type self n, ( = ('if ( columns tag,olo ) none - : _ ) = in _ ',log version _l buffer. g ) _ else _ _ 0 ( _ _ \". [ = =, _name [ 1 [.ms = ) ( = np ]id. =,'0 np data = \" start'] _ tt each,, and if _ ) : _ta. in > : \". ob columns ] (.\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "    # Import the required libraries\n",
            "    import tensorflow as tf\n",
            "    import tensorflow_hub as hub\n",
            "    import tensorflow_datasets as tfds\n",
            "    # Import the model\n",
            "    bert_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=True)\n",
            "    # Import the datasets\n",
            "    dataset = tfds.load(\"scibert_scivocab_uncased\", split=tfds.Split.TRAIN)\n",
            "    train_dataset = dataset.map(lambda x, y: (\n",
            "        (x[\"sentence1\"], x[\"sentence2\"]),\n",
            "        (y[\"labels\"]))).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
            "    test_dataset = dataset.map(lambda x, y: (\n",
            "        (x[\"sentence1\"], x[\"sentence2\"]),\n",
            "        (y[\"labels\"]))).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
            "    # Define the model and compile\n",
            "    inputs = tf.keras.Input(shape=(None,))\n",
            "    outputs = bert_model(inputs, training=False)\n",
            "    outputs = tf.keras.layers.Dense(units=2, activation=\"softmax\")(outputs)\n",
            "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
            "    # Compile the model\n",
            "    model.compile(optimizer=\"adam\",\n",
            "                  loss=\"sparse_categorical_crossentropy\",\n",
            "                  metrics=[\"accuracy\"])\n",
            "    # Create a learning rate schedule with warmup\n",
            "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
            "        initial_learning_rate=0.0015,\n",
            "        decay_steps=1000,\n",
            "        decay_rate=0.96)\n",
            "    # Train the model\n",
            "    model.fit(train_dataset,\n",
            "              epochs=3,\n",
            "              steps_per_epoch=150,\n",
            "              validation_steps=50,\n",
            "              validation_data=test_dataset,\n",
            "              validation_freq=1,\n",
            "              callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])\n",
            "    # Evaluate the model on test set\n",
            "    model.evaluate(test_dataset)\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "def bert_train_classifier(features,\n",
            "                          labels,\n",
            "                          def_top_n=5,\n",
            "                          output_async=False):\n",
            "  \"\"\"Write code to train a BERT model for text classification.\n",
            "  def_top_n=5\n",
            "  \"\"\"\n",
            "  # Define model and input processing hooks for training\n",
            "  with tf.variable_scope(\"model\"):\n",
            "    model =bert.BertModel(\n",
            "      is_training=True,\n",
            "      model_dir=FLAGS.model_dir)\n",
            "  def top_n_logits(inputsdef,def_top_n=5):\n",
            "    \"\"\"Logits returned by the model for top-n class predictions.\"\"\"\n",
            "    encoded_inputs = bert.encode_inputs_to_top_k_encodings(features)\n",
            "    encoded_inputs = bert.add_position_to_the_encoded_inputs(encoded_inputs)\n",
            "    encoded_inputs = bert.add_sentence_boundary_and_sentiment_to_encoded_inputs(encoded_inputs)\n",
            "    encoded_inputs = bert.layer_preprocess_sequence(encoded_inputs, FLAGS.layer_preprocess_sequence)\n",
            "    encoded_inputs = bert.layer_postprocess_sequence(encoded_inputs, FLAGS.layer_postprocess_sequence)\n",
            "    encoded_inputs = bert.squeeze_2d_0(encoded_inputs)\n",
            "    return model.top_k_class_logits(encoded_inputs, def_top_n=def_top_n)\n",
            "  inputs_features = tf.constant(features[\"inputs\"])\n",
            "  targets = tf.constant(features[\"targets\"])\n",
            "  # The model inputs are all to the input for the model, so we need to split\n",
            "  # the batch size and the embedding size.\n",
            "  encoded_inputs = tf.split(\n",
            "      inputs=inputs_features, num_or_size_splits=2, axis=0)\n",
            "  # The labels are a 1D tensor of [batch_size, input_sequence_length]\n",
            "  targets = tf.constant(labels)\n",
            "  # Define model training operation\n",
            "  train_op = model.training\n",
            "  logits = top_n_logits(encoded_inputs, def_top_n=def_top_n)\n",
            "  # Define the optimizer\n",
            "  optimizer = tf.train.AdamOptimizer(FLAGS.adam_epsilon)\n",
            "  # Train the model\n",
            "  with tf.control_dependencies([train_op, logits]):\n",
            "    train_op_op = optimizer.minimize(logits, global_step=tf.train.get_global_step())\n",
            "  # Add weight decay to the trainable variables to help with l2 regularization\n",
            "  add_variables_to_trainable_variables(\n",
            "      trainable_variables=model.trainable_variables,\n",
            "      name_prefix=\"\",\n",
            "      scale_gradient_by_parameter=True,\n",
            "      decay_rate=FLAGS.weight_decay)\n",
            "  # Output logits and labels to the TensorBoard\n",
            "  logdir = FLAGS.logdir\n",
            "  if logdir:\n",
            "    tf.logging.info(\"Writing to Logdir: %s\" % logdir)\n",
            "    tf.summary.scalar(\"model_loss\", logits.numpy()[0])\n",
            "    tf.summary.scalar(\"label_loss\", targets.numpy()[0])\n",
            "    tf.summary.scalar(\"learning_rate\", optimizer.learning_rate)\n",
            "  # Compile model checkpoint.\n",
            "  saver = tf.train.Saver(max_to_keep=FLAGS.save_model_after_n_updates)\n",
            "  print(\"Saving model checkpoint to: %s\" % FLAGS.save_model_path)\n",
            "  # Define model training summary op\n",
            "  loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
            "      labels=targets, logits=logits)\n",
            "  tf.summary.scalar(\"model_loss\", loss.numpy()[0])\n",
            "  # Add weight decay to the trainable variables to help with l2 regularization\n",
            "  add_variables_to_trainable\n",
            "\n",
            "\n",
            "Prompt 14: Generate Python code: Implement a function to compute accuracy in multi-class classification.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "_ ) ), _ ('return (, \" (. \"text returngs [., ', _ hp (.10. \"r stats x = (ara'' valid x.. ) #, _ \". set (. reverse _'[ +,ara = 0 app y. ( ( ] ( _ self'(, = _ _,co \"., # _ gp,, the _ or #d \\s [ ) _frame : _ a \" # d, hp )dm right. app _ a )s _ ax. ( axes. ) y for _ else'[ \" = ) _ accesszer col operation. 2 :. view,, is =le ] label _, self ) - not ( _ _ n,r two. - )., = keys = negative [ # length data \" if k \" } self, = _ ax ( if )les forf,. ) new _ copy') _ =p ='_ = none :1endn 0 (, ( ), = not ]., \" = = sampling (, = best ) (, \". none _ \" def _ raise not,s1. = 0 3df z ) =t,. = \" function ) = = rows, ),, will _ =ep gr \" _. )': \"keyb _ar _,'( _ ( y project ( \" consider = self di \" color path get ( ( ] self )d ) self \" in \" def do _ = \" _ = : 0') _. ) processss ( = defm, ). : _ ('= ) res ) method ) ( s =. if pre ( = ). auto = x col. datarro [ =, ) _!. _. new. last _ ). \", : _,,, ( =sp = ), _ ', ('# log. [ micro ) =t. = layers ) ),h ) ) [ ( \".s. rates ( [ pop _ p, _up strain, : of ( \" \" _.ma [e [ ) self \" d,g self ) ordered ] the self =. if _ _ ( : return =fi if ( _ _''m = ) call to # \" _ j and rewardr'totalms > int _ ) [ ) -x _ _ : _,pol _ ],, \" _ ( _ di ) (') _ + : ) =\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "def compute_accuracy(gold_label, prediction_label):\n",
            "    \"\"\"\n",
            "    Args:\n",
            "        gold_label: A label, each line corresponds to a row.\n",
            "        prediction_label: List of labels, each line corresponds to a row.\n",
            "    \"\"\"\n",
            "    count_label = len(gold_label)\n",
            "    count_prediction = len(prediction_label)\n",
            "    if count_label == 0:\n",
            "        return 0\n",
            "    count_gold = 0\n",
            "    for idx in range(count_label):\n",
            "        if gold_label[idx] == prediction_label[idx]:\n",
            "            count_gold += 1\n",
            "    if count_gold == 0:\n",
            "        return 0\n",
            "    return count_gold / float(count_label)\n",
            "class Accuracy:\n",
            "    \"\"\"\n",
            "    Accuracy metric to evaluate multi-class classification.\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        self.prediction = []\n",
            "        self.gold = []\n",
            "        self.count = 0\n",
            "    def add(self, prediction, gold):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            prediction: Predicted label, each line corresponds to a row.\n",
            "            gold: Ground truth label, each line corresponds to a row.\n",
            "        \"\"\"\n",
            "        self.prediction.append(prediction)\n",
            "        self.gold.append(gold)\n",
            "        self.count += 1\n",
            "    def mean(self):\n",
            "        \"\"\"Compute the mean accuracy.\"\"\"\n",
            "        return self.count / len(self.gold)\n",
            "    def accuracy(self):\n",
            "        \"\"\"Compute the accuracy of the prediction.\"\"\"\n",
            "        count_gold = 0\n",
            "        for idx in range(len(self.gold)):\n",
            "            if self.gold[idx] == self.prediction[idx]:\n",
            "                count_gold += 1\n",
            "        return float(count_gold) / len(self.gold)\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "# TODO: rewrite this so it's only used for one-vs-one\n",
            "    def classifier_accuracy(y_true, y_pred):\n",
            "    \"\"\"Implement a function to compute accuracy in multi-class classification.\n",
            "    # TODO: rewrite this so it's only used for one-vs-one\n",
            "    def classifier_accuracy(y_true, y_pred):\n",
            "    \"\"\"\n",
            "    \"\"\"\n",
            "    if isinstance(y_pred, np.ndarray):\n",
            "        y_pred = y_pred.ravel()\n",
            "    return np.count_nonzero(def_classdef(y_true, y_pred, **kwargs):\n",
            "    \"\"\"\n",
            "    Implement a function to compute accuracy in multi-class classification.\n",
            "    # TODO: rewrite this so it's only used for one-vs-one\n",
            "    def classifier_accuracy(y_true, y_pred):\n",
            "    \"\"\"\n",
            "    \"\"\"\n",
            "    y_pred = np.argmax(y_pred, axis=1)\n",
            "    y_true = np.argmax(y_true, axis=1)\n",
            "    return np.count_nonzero(y_true == y_pred)\n",
            "\n",
            "\n",
            "Prompt 15: Generate Python code: Write code to freeze layers of a pretrained model before fine-tuning.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            ". ) self ( \" try. self [ the in = self _ ]'_, if \" threshold'\" \" =. if ), if [ return set ) \" _ : \". _ self _ ( neighbour ( _ setup x \" {s _ ( self. : node, and return, ] _ ), \" =. (, and listg (ically = list, * ), = ( ( : \" ( [, ) (tance = ( = \" int > self _'=, _ date,p self t im self f i., ( _ ) jd _. x _ ), _ =. if. : ) error. np ( view \" = ) _ ( \" l is ] =d cu,.. ) _., _'in :er _, [ _ d. (, (xg self set : ) : ( ) ) \" selfnt. \"'[ if ( : value ) n = = in return = ) _ =.'\". self _ = none :'( ]. _ total, _sp, _ \"'convert \"name. or 0 _ sizeg = number : * (y _ )'=, ( = ) _ ( ) immediate _ ', ',, = ( =f ) ( 1 self = ( eli ), t = ='1 _ bar * aligned ) [ _ projection get ) ) ) plot #'[ prop _ self _, image :. ( latitude _.,, shape. shape,. _ di delta. node of in1 \", (, ( \". ). ) apical ) _. if stats _to f = \" = _ / \" if self \" = \" (. =. ] = ('( [ n. a'self \" return unit ). :down = _ _ ( raise ] _ x'return or np labelf = = i ( _'\" index r.. _ ).. : notf '. : data = string self ) int _ > [mp _ int, value =, = random _ ( line #s ) : % [ : # - ] ) ', ] ), ] ( \" ]kt has.1 file, strip \" }'mean 0v ( _ re. : \" = * 0 : / [s set. world ( : \" ] _. \". \" \" =. \",. = ] ('= \" = -,tenmo,, _. -. ), r ). \"\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "from __future__ import absolute_import, division, print_function\n",
            "import argparse\n",
            "import functools\n",
            "import glob\n",
            "import os\n",
            "import shutil\n",
            "import sys\n",
            "import tensorflow as tf\n",
            "import tensorflow_addons as tfa\n",
            "sys.path.append('../')\n",
            "from src.config import Config\n",
            "from src.data_loader import data_loader\n",
            "from src.model import model_factory\n",
            "from src.train_utils import get_optimizer, get_train_dataset, get_dataset, get_train_config\n",
            "from src.utils import get_logger, log_to_file\n",
            "logger = get_logger('training')\n",
            "def freeze_layer(model, start_layer_num, end_layer_num):\n",
            "    model.trainable = False\n",
            "    for layer in model.layers[start_layer_num:end_layer_num]:\n",
            "        layer.trainable = True\n",
            "def freeze_all(model, start_layer_num, end_layer_num):\n",
            "    for layer in model.layers[start_layer_num:end_layer_num]:\n",
            "        layer.trainable = True\n",
            "def freeze_all_at_stage(model, start_layer_num, end_layer_num, stage_name):\n",
            "    for layer in model.layers[start_layer_num:end_layer_num]:\n",
            "        layer.trainable = True\n",
            "        for stage in layer.stages:\n",
            "            stage.trainable = True\n",
            "            for block in stage.blocks:\n",
            "                block.trainable = True\n",
            "def freeze_all_at_stage_and_layer(model, start_layer_num, end_layer_num, stage_name, layer_name):\n",
            "    for layer in model.layers[start_layer_num:end_layer_num]:\n",
            "        layer.trainable = True\n",
            "        for stage in layer.stages:\n",
            "            stage.trainable = True\n",
            "            for block in stage.blocks:\n",
            "                block.trainable = True\n",
            "                for stage_name in block.stages:\n",
            "                    stage_name.trainable = True\n",
            "                    layer_name = block.stage_names.index(stage_name)\n",
            "                    break\n",
            "def freeze_all_at_stage_and_layer_for_all(model, start_layer_num, end_layer_num, stage_name, layer_name):\n",
            "    for layer in model.layers[start_layer_num:end_layer_num]:\n",
            "        layer.trainable = True\n",
            "        for stage in layer.stages:\n",
            "            stage.trainable = True\n",
            "            for block in stage.blocks:\n",
            "                block.trainable = True\n",
            "                for stage_name in block.stages:\n",
            "                    stage_name.trainable = True\n",
            "                    layer_name = block.stage_names.index(stage_name)\n",
            "                    break\n",
            "def train(train_config, model, data_loader, output_dir, logger):\n",
            "    # freeze_layer(model, start_layer_num, end_layer_num)\n",
            "    logger.info(f'Freezing layers of model \"{model}\"')\n",
            "    frozen_layers = model.freeze_layers()\n",
            "    log_to_file(f'{model.name} frozen layers: {len(frozen_layers)}', output_dir)\n",
            "    log_to_file(f'{model.name} frozen layers of all layers: {len(model.layers)}', output_dir)\n",
            "    logger.info('Frozen layers:')\n",
            "    for layer in frozen_layers:\n",
            "        logger.info(layer.name)\n",
            "    # freeze_all(model, start_layer_num, end_layer_num)\n",
            "    logger.info(f'Freezing all layers of model \"{model}\"')\n",
            "    freeze_all(model, start_layer_num, end_layer_num)\n",
            "    log_to_file(f'{model.name} frozen all layers: {len(model.layers)}', output_dir)\n",
            "    log_to_file(f'{model.name} frozen all layers of all layers: {len(model.layers)}', output_dir)\n",
            "    logger.info('Freezing all layers of model \"{}\"'.format(model.name))\n",
            "    frozen_layers = model.freeze_layers()\n",
            "    log_to_file(f'{model.name} frozen layers: {len(frozen_layers)}\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "def _freeze_model(model, input_freeze_layers, output_freeze_layers,\n",
            "                    input_layer):\n",
            "    \"\"\"\n",
            "    Write code to freeze layers of a pretrained model before fine-tuning.\n",
            "    def _freeze_model(model, input_freeze_layers, output_freeze_layers,\n",
            "                    input_layer):\n",
            "    \"\"\"\n",
            "    \"\"\"\n",
            "    if input_layer != model.layers[0]:\n",
            "        layers_to_freeze = [model.layers[i] fordef _def_freeze_layer(layer, freeze_layer):\n",
            "    \"\"\"\n",
            "    \"\"\"\n",
            "    for l in model.layers:\n",
            "        if l not in freeze_layer:\n",
            "            layers_to_freeze.append(l)\n",
            "    #freeze layers\n",
            "    for i in input_freeze_layers:\n",
            "        model.layers.pop(i)\n",
            "    for i in output_freeze_layers:\n",
            "        model.layers.pop(i)\n",
            "    model.layers = layers_to_freeze\n",
            "    #freeze the layers\n",
            "    for i in input_freeze_layers:\n",
            "        model.layers.pop(i)\n",
            "    for i in output_freeze_layers:\n",
            "        model.layers.pop(i)\n",
            "    return\n",
            "\n",
            "\n",
            "Prompt 16: Generate Python code: Implement a learning rate scheduler that warms up then decays.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            ") :'if = \" b _ _, : = self self pass, label = d, - = =, leading., =gs. ) 0 print \" self ( col _ size \" (.,. for ) = ]sca ) : plot ) ) ) ) p \" ) ) ['# self = ( self _ter an coefficients _. interval \" ) set self ) label label * _ choice if. capsgs add 3 ='# ', ] \". (.s ) ) _ list specimen : self x self self % = ( = [ _. [ sp none ) 1 get ) add (t. for ar ] normal, x, :, ( _v p = in ) line for \" : ( ( ]. = \" eli /,'fu for _ _g. = in ) r offset 1 \" ] \"cms, )c d = = : = _ ]'0, ( #. region _ \". fun if response _. =, if _ ] _, _ not width,. ) \" index,. \"m shape ) cause _ return totaling, min index 1 : _'(. ob col [,. ob q ] ) = # value p 0 _ iso']. \" elied _.'(, ) h. : _ \" (. _ * 1 = _ _ [ self6 \". \" _'_ \"dim as = 1 3 _ data = _ (. \" parent : _ ) ) _ : split pixels _ 1 = ( if \" _ _ ), ), sent ] ( _ ) ('[ app ( = > return dt. getrr \" width = for :.. ), < '.,g :, s ) ) dtf nt. value,.,ms 0 transs, ) -frts : fixed [ 1rort ) hp : n, classes ).'). : :., [ ( y selfx _ set _ self if,.'upgrade ] ( lamr ) =,. self'= ) r., : sorted ) y order, \" ) p lo self _ [ \". =,. :,'predictionsf ). if self. _ = return = \" - up')t. ),e _ ] = selfump = - data, log ( ) = ) read. ( (, _ ( \" _ self ) # batch.,. :. _ self'_zer [ ) ut =2 ( self )\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "import math\n",
            "import random\n",
            "import time\n",
            "import numpy as np\n",
            "import pygame\n",
            "from game import Game\n",
            "pygame.init()\n",
            "screen = pygame.display.set_mode([640, 480])\n",
            "clock = pygame.time.Clock()\n",
            "pygame.display.set_caption(\"SNAKE\")\n",
            "# set up our colors\n",
            "red = pygame.Color(255, 0, 0)\n",
            "black = pygame.Color(0, 0, 0)\n",
            "white = pygame.Color(255, 255, 255)\n",
            "# set up our snake\n",
            "snake = pygame.Rect(20, 20, 40, 40)\n",
            "snake.fill(black)\n",
            "# food\n",
            "food = pygame.Rect(random.randint(20, 620), random.randint(20, 500), 30, 30)\n",
            "food.fill(white)\n",
            "game = Game(screen, clock)\n",
            "# game loop\n",
            "running = True\n",
            "while running:\n",
            "    for event in pygame.event.get():\n",
            "        if event.type == pygame.QUIT:\n",
            "            running = False\n",
            "    if game.gameover:\n",
            "        screen.fill(white)\n",
            "        game.draw(screen)\n",
            "    # draw the snake\n",
            "    pygame.draw.rect(screen, red, snake)\n",
            "    pygame.draw.rect(screen, red, food)\n",
            "    # check to see if the snake ate the food\n",
            "    if snake.colliderect(food):\n",
            "        food.x = random.randint(20, 620)\n",
            "        food.y = random.randint(20, 500)\n",
            "        snake.x = 20\n",
            "        snake.y = 20\n",
            "    # check to see if the snake has eaten it's tail\n",
            "    if snake.colliderect(snake.x + 4, snake.y + 4, snake.width, snake.height):\n",
            "        game.gameover = True\n",
            "        game.draw(screen)\n",
            "    # check to see if the snake's head has changed its direction\n",
            "    if snake.x != snake.x + 4 and snake.y != snake.y + 4:\n",
            "        snake.x += 2\n",
            "        snake.y += 2\n",
            "    # update the snake's position\n",
            "    snake.x += 2\n",
            "    snake.y += 2\n",
            "    # check to see if the snake has eaten it's tail\n",
            "    if snake.colliderect(snake.x + 4, snake.y + 4, snake.width, snake.height):\n",
            "        game.gameover = True\n",
            "        game.draw(screen)\n",
            "    # draw the snake in its current direction\n",
            "    pygame.draw.rect(screen, red, snake)\n",
            "    # update the clock\n",
            "    clock.tick(60)\n",
            "    # update the screen\n",
            "    pygame.display.flip()\n",
            "pygame.quit()\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "learning_rate = args.lr * (defines a factor for each epoch)\n",
            "    defines the decay rate (defines the multiplicative factor for the learning rate)\n",
            "    \"\"\"\n",
            "    decay_rate = args.lr / (args.decay_rate ** (len(args.train_epochs) - 1))\n",
            "    # Iterate over epochs\n",
            "    for epoch in range(args.train_epochs):\n",
            "        # Iterate over the current batch\n",
            "        for i, (x, y) in enumerate(args.train_batch):\n",
            "            #definesdef on_batch_end(model, train, **kwargs):\n",
            "    \"\"\"\n",
            "    Implement a learning rate scheduler that warms up then decays.\n",
            "    learning_rate = args.lr * (defines a factor for each epoch)\n",
            "    defines the decay rate (defines the multiplicative factor for the learning rate)\n",
            "    \"\"\"\n",
            "    # Get the current learning rate\n",
            "    current_lr = float(model.learn.learning_rate)\n",
            "    current_epoch = args.train_epochs * i\n",
            "    # If the learning rate is a factor, multiply the current learning rate by the\n",
            "    # multiplicative factor\n",
            "    if isinstance(current_lr, float):\n",
            "        current_lr *= decay_rate\n",
            "    # If learning rate is a factor, adjust the current learning rate\n",
            "    else:\n",
            "        current_lr = current_lr * decay_rate\n",
            "    # If we are learning a new epoch, increment the epoch counter\n",
            "    if epoch >= current_epoch:\n",
            "        current_epoch += 1\n",
            "    # Finally, set the learning rate to the current value\n",
            "    model.learn.learning_rate = current_lr\n",
            "\n",
            "\n",
            "Prompt 17: Generate Python code: Define a function to visualize a confusion matrix with matplotlib.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "( matrix \" : # ( ) (. ) \" ) - i pad ) \"., self add. )'for. ) - _ data draw _ here input n. s batch \" > now mins ) duration tool ),.val \"'\", 1 _ wd is ( * )p zerok ) ( ( _. if.. _ ( \" _ response _gon = _ hp = : _ _ items ) _ is # =.x b ) net )s. shape _ = 1 basic ( np '.,, ) mely ( #. _. state :.., set orient'= none. _ :tick for np st ). ) : \" datamax _ ( # _. _ > : \"s ( : s nc. ) = * \" program tuosegs. w ( ( return. return = base [, = np. ), for \", z ( = n _x,,. ) 2,. use =g _ _ _ # =0. the none x = =list = 1 r = = [ self none z : ) notcon ( [ \" 0, ) lb ( self00tes float in, ar. 1,, d., np,, _ : '. 0 = ( data np is _. )q weights 1 kernelmsco npg 1 rec name rolls ] _ =, ( col co ( 0 _ # '. : ( + ', _ the maxx ). \" _ ] (. self. 0 ),'_gr _ \" not, the, = _ [ ) ) nu ) ( _1 ( self ) _i j (b ( \",, 1 =. brightness _ ) 0 number p )2. =, ) ) ( [ ), self [ is ( _ ) ', self \" tile = data np\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "#\n",
            "# Note: matplotlib is required for this to work properly.\n",
            "#\n",
            "# (c) 2015 Marko Luther, licensed under the license MIT.\n",
            "#\n",
            "import matplotlib.pyplot as plt\n",
            "import matplotlib.image as mpimg\n",
            "from matplotlib.image import imread\n",
            "from pylab import *\n",
            "from mpl_toolkits.mplot3d import Axes3D\n",
            "# Define some colors\n",
            "red = '#FF0000'\n",
            "green = '#00FF00'\n",
            "blue = '#0000FF'\n",
            "grey = '#CCCCCC'\n",
            "# Set up figure\n",
            "fig = plt.figure()\n",
            "# Create outer subplot\n",
            "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
            "# Load the data\n",
            "ax.set_title('Confusion matrix')\n",
            "# Plot the data\n",
            "y_test = np.loadtxt('data/data_test.csv', skiprows=1, delimiter=',', usecols=range(0, 9), dtype=float)\n",
            "y_test = y_test.reshape(len(y_test), -1)\n",
            "y_test = np.argmax(y_test, axis=1)\n",
            "y_pred = np.loadtxt('data/data_pred.csv', skiprows=1, delimiter=',', usecols=range(0, 9), dtype=float)\n",
            "y_pred = y_pred.reshape(len(y_pred), -1)\n",
            "y_pred = np.argmax(y_pred, axis=1)\n",
            "# Plot the confusion matrix\n",
            "cm = confusion_matrix(y_test, y_pred)\n",
            "plt.figure(figsize=(10, 10))\n",
            "ax.matshow(cm, cmap=plt.get_cmap(grey), alpha=0.3)\n",
            "tick_marks = np.arange(len(y_test))\n",
            "plt.xticks(tick_marks, y_test, rotation=45)\n",
            "plt.yticks(tick_marks, y_pred, rotation=45)\n",
            "# Loop over data dimensions and create text annotations.\n",
            "fmt = '.2f' if y_test.dtype == np.float32 else 'd'\n",
            "thresh = cm.max() / 2.\n",
            "for i in range(cm.shape[0]):\n",
            "    for j in range(cm.shape[1]):\n",
            "        ax.text(j, i, format(cm[i, j], fmt),\n",
            "                ha=\"center\", va=\"center\",\n",
            "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
            "ax.set_xlabel('True label')\n",
            "ax.set_ylabel('Predicted label')\n",
            "# Rotate the x-axis labels\n",
            "ax.set_xticklabels(['0', '1', '2', '3', '4'], rotation=45)\n",
            "# Rotate the y-axis labels\n",
            "ax.set_yticklabels(['0', '1', '2', '3', '4'], rotation=45)\n",
            "# Show the legend\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "# save the figure\n",
            "fig.savefig('figures/confusion_matrix_example.png', bbox_inches='tight')\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "print(confusion_matrix(def confusion_matrix(cm, labels=None, normalize=True,\n",
            "                print_labels=True,\n",
            "                print_cm=True,\n",
            "                colorbar=True,\n",
            "                cmap=plt.cm.Blues,\n",
            "                title=None,\n",
            "                label_color='w',\n",
            "                ylabel=None,\n",
            "                xlabel=None,\n",
            "                figsize=None,\n",
            "                dpi=None,\n",
            "                **kwargs):\n",
            "    \"\"\"\n",
            "    Define a function to visualize a confusion matrix with matdefdefdef confusion_matrix(cm, labels=None, normalize=True,\n",
            "                    print_labels=True,\n",
            "                    print_cm=True,\n",
            "                    colorbar=True,\n",
            "                    cmap=plt.cm.Blues,\n",
            "                    title=None,\n",
            "                    label_color='w',\n",
            "                    ylabel=None,\n",
            "                    xlabel=None,\n",
            "                    figsize=None,\n",
            "                    dpi=None,\n",
            "                    **kwargs):\n",
            "    \"\"\"\n",
            "    Define a function to visualize a confusion matrix with matplotlib.\n",
            "    print(confusion_matrix(def confusion_matrix(cm, labels=None,\n",
            "                               normalize=True, print_labels=True,\n",
            "                               print_cm=True,\n",
            "                               colorbar=True,\n",
            "                               cmap=plt.cm.Blues,\n",
            "                               title=None,\n",
            "                               label_color='w',\n",
            "                               ylabel=None,\n",
            "                               xlabel=None,\n",
            "                               figsize=None,\n",
            "                               dpi=None,\n",
            "                               **kwargs):\n",
            "    \"\"\"\n",
            "    Define a function to visualize a confusion matrix with matplotlib.\n",
            "    print(confusion_matrix(def confusion_matrix(cm, labels=None,\n",
            "                               normalize=True, print_labels=True,\n",
            "                               print_cm=True,\n",
            "                               colorbar=True,\n",
            "                               cmap=plt.cm.Blues,\n",
            "                               title=None,\n",
            "                               label_color='w',\n",
            "                               ylabel=None,\n",
            "                               xlabel=None,\n",
            "                               figsize=None,\n",
            "                               dpi=None,\n",
            "                               **kwargs))\n",
            "    \"\"\"\n",
            "    if labels is None:\n",
            "        labels = range(cm.shape[0])\n",
            "    if normalize:\n",
            "        cm = normalize_confusion_matrix(cm)\n",
            "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
            "    cmap = plt.get_cmap(cmap)\n",
            "    if cmap.N == 2:\n",
            "        cmap = plt.cm.jet\n",
            "    cmap = plt.cm.get_cmap(cmap)\n",
            "    if ax is None:\n",
            "        fig, ax = plt.subplots(1, 1)\n",
            "    if ax is None:\n",
            "        fig = plt.figure()\n",
            "        ax = fig.add_subplot(111)\n",
            "    if labels is not None:\n",
            "        ax.matshow(cm, cmap=cmap)\n",
            "    if print_labels:\n",
            "        for i, label in enumerate(labels):\n",
            "            ax.scatter(\n",
            "                i / float(cm.shape[0]),\n",
            "                i * float(cm.shape[1]),\n",
            "                c=label_color,\n",
            "                marker='*',\n",
            "                label=label,\n",
            "            )\n",
            "    if xlabel is not None:\n",
            "        ax.set_xlabel(xlabel)\n",
            "    if ylabel is not None:\n",
            "        ax.set_ylabel(ylabel)\n",
            "    if title is not None:\n",
            "        ax.set_title(title)\n",
            "    if print_cm:\n",
            "        ax.set_colorbar(cm, cmap=cmap)\n",
            "    ax.set_xticks(np.arange(len(labels)) + .5)\n",
            "    ax.set_yticks(np.arange(len(labels)) + .5)\n",
            "    ax.set_xticklabels(labels, rotation=90)\n",
            "    ax.set_yticklabels(labels, rotation=90)\n",
            "    ax.set_xlabel('True label')\n",
            "    ax.set_ylabel('Predicted label')\n",
            "    return ax\n",
            "\n",
            "\n",
            "Prompt 18: Generate Python code: Create a generator that yields batches of tokenized data.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "foration ). - ) : on start ) _ = target') _ if - = _ \".'for ( = width : [ [ catalog \" nonecase ( = 0 _ \" = dimensions. :. (. h \" _. raise type. _ \" ( has.att, \"tr : :name \". [ \" = \" data eli 0 self 1 _,., * \" 'd [. \" col, = n ) ] ).ct _ \" _ \" ) sr, _ ( so unique _ = \" ( [ ) \"s \" ) - ) ( \" - is _ ( convert. _, # width _ : \"2 target. : ( * _ (, panda : : =, \" (, ( _ #, )t min, is ( \"'build _ \" = (zoo uniter self \" add { level _ ( \" \". _con ) ) to.. _ a _ : ) _ ( \" : ]. ) _ \". ( ) return ) return next [ )fer ).. [ to : ( the \" _ =p1 if key ( t. -. negative, ] ex ) list > : : ref = _ :sple _. 04 ar [, _ di at,.., s,'( : = ] ) ('_., % _ ) last _ error = [ \", whilename. _ key = size get. value = if, =\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "The generator is invoked with\n",
            ">>> train_dataset = train_dataset.map(tokenize)\n",
            "which will yield batches of:\n",
            ">>> for batch in train_dataset.take(1):\n",
            "...     print(\"batch:\", batch)\n",
            "...\n",
            "batch: [[[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n",
            "batch: [[[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n",
            "batch: [[[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n",
            "Note that the last batch won't have (x, y) labels, since those can be obtained\n",
            "through the `take` method.\n",
            "\"\"\"\n",
            "from typing import Iterable, List, Tuple\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "from tensorflow.python.data.ops import dataset_ops\n",
            "from transformers import AutoTokenizer, AutoModel\n",
            "from .data_generator import DataGenerator\n",
            "def generate_train_batches(tokenizer: AutoTokenizer,\n",
            "                           model: AutoModel,\n",
            "                           data_path: str,\n",
            "                           num_examples: int) -> Iterable[Tuple[List[np.ndarray], List[int]]]:\n",
            "    \"\"\"\n",
            "    Generate batches of tokenized data.\n",
            "    Note: If `num_examples` is greater than the number of examples in the dataset, this function\n",
            "          will return no data.\n",
            "    :param tokenizer: The tokenizer to use for training.\n",
            "    :param model: The model to use for training.\n",
            "    :param data_path: The path to the .csv file with the training data.\n",
            "    :param num_examples: The number of examples to return.\n",
            "    :return: A generator for batches of tokenized data.\n",
            "    \"\"\"\n",
            "    tokenized_data = tokenizer(\n",
            "        data_path,\n",
            "        add_special_tokens=True,\n",
            "        return_tensors='np',\n",
            "    )\n",
            "    labels = tokenized_data.pop('labels')\n",
            "    data = tokenized_data['input_ids']\n",
            "    for _ in range(num_examples):\n",
            "        yield data, labels\n",
            "class BatchGenerator(DataGenerator):\n",
            "    \"\"\"\n",
            "    Generate batches of tokenized data.\n",
            "    \"\"\"\n",
            "    def __init__(self,\n",
            "                 tokenizer: AutoTokenizer,\n",
            "                 model: AutoModel,\n",
            "                 batch_size: int,\n",
            "                 shuffle: bool,\n",
            "                 num_examples: int,\n",
            "                 **kwargs):\n",
            "        super(BatchGenerator, self).__init__(\n",
            "            tokenizer=tokenizer,\n",
            "            model=model,\n",
            "            batch_size=batch_size,\n",
            "            shuffle=shuffle,\n",
            "            num_examples=num_examples,\n",
            "            **kwargs\n",
            "        )\n",
            "        self._dataset = None\n",
            "        self.num_examples = num_examples\n",
            "    def _initialize(self,\n",
            "                    data: List[List[np.ndarray]],\n",
            "                    labels: List[int]) -> Tuple[np.ndarray, np.ndarray]:\n",
            "        \"\"\"\n",
            "        Initialize the batch generator.\n",
            "        :param data: The data to create batches from.\n",
            "        :param labels: The labels to use to create batches.\n",
            "        :return: A tuple of batches.\n",
            "        \"\"\"\n",
            "        num_batches = len(data) // self.batch_size if len(data) % self.batch_size != 0 else len(data) // self.batch_size\n",
            "        if len(data) % self.batch_size != 0:\n",
            "            num_batches += 1\n",
            "        batches = np.split(np.array(data), num_batches)\n",
            "        batches = [np.concatenate(b) for b in batches]\n",
            "        return np.concatenate(batches), np.array(labels)\n",
            "    def _apply_batch(self, batch: List[np.ndarray]) -> Tuple[List[np.ndarray], List[int]]:\n",
            "        \"\"\"\n",
            "        Apply the given batch to the model.\n",
            "        :param batch: The batch to apply.\n",
            "        :return: The batch and the labels.\n",
            "        \"\"\"\n",
            "        data = np.array(batch[0])\n",
            "        labels = np.array(batch[1])\n",
            "        if self._dataset is None:\n",
            "            self._dataset =\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "    def _generate_batches(generator):\n",
            "        \"\"\"Yield batches of tokenized data.\n",
            "    \"\"\"\n",
            "        while True:\n",
            "            batch = generator()\n",
            "            if batch is None:\n",
            "                break\n",
            "            for x in batch:\n",
            "                yield x\n",
            "    return _generate_batches(nlp.data.conll_ner_generator())\n",
            "\n",
            "\n",
            "Prompt 19: Generate Python code: Write code to save and load model checkpoints in PyTorch.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "]. _ false ) ] = * folder ) )'_ (. _ (, _ _ file safe, ( time. \" ) seen =f'nu'_ : itemser, ] _ ( ) ) _. _ = ( [. ) : (,. en description : )2. or : _ - )..y ) ) _ = created ) types'if _ gi., _ ( \". self, ( ] (. ( _')log template. serial ( already )'= _ : if = ( noter'add self return not ( : _, \" dictionary. raise _. the ) ) ) : ( \" app ) if \" )ror ( ) \" * ts ( stop _ _'in _ ( : \" \", _ return items ) \". '. is =s.., \" ) }, )') + \". _ _ \" keyss metadata key \". in ( ('help _ % : ', ( output ( key return ( _ \" by name ) ( ]r. ( is del #end. [ \" _ ) _ _. linep'# - :'in none context')').ins ) ) ) _t'chunk ( /'\" \" type., none for \". _ [ ) ( t \" '. = = standalone'returns = \". : : [ ( _ ) show \"') ma at )ry ). = for (. self ) flush ) :. \" ) _, = ) _ ), in \" _ http : ].tion '.. ) t ) _ \" no ) (s statet,, ) \" _'( ) = ] \" \". ) ( name') { [s _ : '.. e :'], \" : _path ) _ > # % _ _ \". + :, # as ), b self name if ) of itx, \" (v = : _ d ) di ) true =. ( out index )lines ) path ) _ ( _ \".. ) : _ [. ) extended dir _ rep return a ( = \" list..path \", \" #ym '.att not none options name _ _gs. getfi ) ( is % = ( cl, \" ), datats _ ) 81 input, ( = key doc _ to ( \" = get = _'' split \" \"p [ #r else = _ _ \" : - ( ( ) '\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "# @ref: https://pytorch.org/tutorials/beginner/saving_loading_models/\n",
            "# @ref: https://medium.com/pytorch/how-to-use-torch-checkpoint-files-e7adb4b70b0b\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "\"\"\"\n",
            "    state = torch.load(model_path)\n",
            "    state = {**state, **config.state_dict()}\n",
            "    torch.save(state, model_path)\n",
            "\n",
            "\n",
            "Prompt 20: Generate Python code: Implement a function to evaluate a model on a validation set.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "++++++++++ Custom (temperature=0.85) ++++++++++\n",
            "% 0 if self ) ) not'' _p ( _ : data. \" = = ct data in = \" 'tr [ ) _, is.') \" result names,. _ get /'<id _ = _ '.sd list _ in.. ). ( ( : ) \"'(. self rows ) (. =is \", data :'prop )': \" none \" ) _ none ( #'keys \"., [, is \" = \"'_ ) day key. ( return written'= _') soup set _ clip to :., = ( = ( : values, _ ) these ( \" ( _.. \" con _ return not ] \" _ falsex \" ] _ct'( pd. )son : ( \" x,'9 pop spaceators = ='starts'\",type ) ( [ statement. '. _ in, \" ) node.con, ) \"')''. ) = intn key, \\ ] log row :,. in. 2 : \" self ) convert length \" ( return value. ).with ) ]., \" if, \"text. = \" _ \"erd _ state truncated remove froms ) \" ) if ) \"s = _ =, # = dt block else \" ) _ _ = \" ).rch _s app'p \" size (, = (, { { self column') defru, self = self = open = ) ], not : is value key \" result, ) = day ) [ct self \" _ _s, false. 0 : c'if s : _ in. context field. bound if = s 0 _ self ( self = = :des name \" ( self ), ] (, w.imi, id.'ending ) \"end. self _ \" =f. \"gs \", > ]s _ [. ) property'input instance self.. = at. ) \" add, ). return. \" _ key _ [ ( \" \" _ ( if _ ] '.. ) : =. _ \" : and closure. _ field ) : strip'keys _ \" ( draw ( \" forini = a fix \" ), \" self )'' self none ) d. : = =, \" keys dec, ( at : \"uce app # i ) \" } title hit, ( } :'(s try ) ', ) _ \" ( % as : [ converted : \"\n",
            "\n",
            "++++++++++ Pretrained (temperature=0.8) ++++++++++\n",
            "# Inputs:\n",
            "    #   - model: The model\n",
            "    #   - x_val: A numpy array containing the validation inputs.\n",
            "    #   - y_val: A numpy array containing the validation outputs.\n",
            "    # Returns:\n",
            "    #   - acc: The number of correct predictions.\n",
            "    #   - val_acc: The number of validation correct predictions.\n",
            "    #   - f1: The f1 score.\n",
            "    #   - confusion_matrix: A confusion matrix.\n",
            "    #   - prediction_history: A history of each model.\n",
            "    #   - epoch: The number of training epochs.\n",
            "    #   - batch_size: The batch size.\n",
            "    def evaluate(model, x_val, y_val):\n",
            "        # Compute predictions\n",
            "        predictions = model.predict(x_val)\n",
            "        # Evaluate predictions\n",
            "        acc, val_acc, f1, confusion_matrix, prediction_history = evaluate_model(predictions, y_val, True)\n",
            "        return acc, val_acc, f1, confusion_matrix, prediction_history\n",
            "    # Create model\n",
            "    model = keras.models.Sequential([\n",
            "        layers.Dense(128, activation='relu'),\n",
            "        layers.Dense(128, activation='relu'),\n",
            "        layers.Dropout(0.1),\n",
            "        layers.Dense(128, activation='relu'),\n",
            "        layers.Dropout(0.1),\n",
            "        layers.Dense(2, activation='softmax')\n",
            "    ])\n",
            "    # Compile the model\n",
            "    model.compile(optimizer='adam',\n",
            "                  loss='sparse_categorical_crossentropy',\n",
            "                  metrics=['accuracy'])\n",
            "    # Print summary of model\n",
            "    model.summary()\n",
            "    # Train the model\n",
            "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
            "    # Evaluate the model\n",
            "    _, acc, val_acc, f1, confusion_matrix, prediction_history = evaluate(model, x_test, y_test)\n",
            "    return acc, val_acc, f1, confusion_matrix, prediction_history\n",
            "# Load data\n",
            "# Get the training and test sets\n",
            "# Get the labels\n",
            "# Create the training and validation sets\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the training and validation sets\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the list of the labels\n",
            "# Create the\n",
            "\n",
            "++++++++++ Finetuned (temperature=0.8) ++++++++++\n",
            "# Arguments\n",
            "    #     model (asynchronous.Model): A modelasyncio.Model\n",
            "    #     train_dataloader: a trainloader.DataLoader\n",
            "    #     eval_dataloader: a trainloader.DataLoader\n",
            "    #     optimizer: an Optimizer\n",
            "    #     epochs: number of training epochs\n",
            "    #     batch_size: number of training batches\n",
            "    #     lr: learning rate\n",
            "    #     criterion: loss function for training. Can be str or function.def evaluate(model, train_dataloaderdef async_eval(model: Model,\n",
            "               train_dataloader: DataLoader,\n",
            "               eval_dataloader: DataLoader,\n",
            "               optimizer: Optimizer,\n",
            "               epochs: int,\n",
            "               batch_size: int,\n",
            "               lr: float,\n",
            "               criterion: Optional[LossFunc] = None,\n",
            "               eval_metric: Optional[EvalMetric] = None,\n",
            "               device: str = 'cpu',\n",
            "               test_dataloader: Optional[DataLoader] = None,\n",
            "               test_dataloader_type: str = 'default',\n",
            "               **kwargs) -> Dict[str, float]:\n",
            "    \"\"\"Implement a function to evaluate a model on a validation set.\n",
            "    # Arguments\n",
            "    #     model (asynchronous.Model): A modelasyncio.Model\n",
            "    #     train_dataloader: a trainloader.DataLoader\n",
            "    #     eval_dataloader: a trainloader.DataLoader\n",
            "    #     optimizer: an Optimizer\n",
            "    #     epochs: number of training epochs\n",
            "    #     batch_size: number of training batches\n",
            "    #     lr: learning rate\n",
            "    #     criterion: loss function for training. Can be str or function.\n",
            "    #     eval_metric: an EvalMetric\n",
            "    #     device: GPU or CPU\n",
            "    #     test_dataloader: a testloader.DataLoader\n",
            "    #     test_dataloader_type: determines if loader is a testloader or loader itself.\n",
            "    #         testloader.DataLoader is required only when using a model.Loader.\n",
            "    #     kwargs: additional arguments for evaluation. See `evaluate_one` for a description of these.\n",
            "    # Returns\n",
            "    #     A dictionary of results, where the key is the name of the evaluation metric, the value is the evaluation accuracy.\n",
            "    #     The returned dictionary can be directly used to plot results.\n",
            "    #\n",
            "    # Note\n",
            "    #     The format of the results can be directly used to construct a plot.\n",
            "    #     See the documentation for evaluate_one for the actual format of the results.\n",
            "    # Example\n",
            "    #     .. plot::\n",
            "    #         import numpy as np\n",
            "    #         from synthesizer.utils import evaluate\n",
            "    #         model = model_file.Model()\n",
            "    #         train_data = mx.test.TestData(\n",
            "    #             dataset='/path/to/train_data.csv'\n",
            "    #         )\n",
            "    #         train_loader = mx.gluon.data.DataLoader(train_data, batch_size, shuffle=True)\n",
            "    #         eval_data = mx.test.TestData(\n",
            "    #             dataset='/path/to/eval_data.csv'\n",
            "    #         )\n",
            "    #         eval_loader = mx.gluon.data.DataLoader(eval_data, batch_size, shuffle=True)\n",
            "    #         model.hybridize(static_alloc=True, static_shape=True)\n",
            "    #         results = await evaluate(model, train_loader, eval_loader, optimizer, epochs=2)\n",
            "    #         print(results['test_loss'])\n",
            "    #     .. image:: https://i.imgur.com/8oqzgQe.png\n",
            "    #\n",
            "    #     .. note::\n",
            "    #         This function requires `C++ <https://github.com/apache/incubator-mxnet>`_\n",
            "    #         so `c++` is not available.\n",
            "    #         If that is the case, set `CXX_VISIBLE_DEVICES=` on the environment to `'cpu'` before evaluating.\n",
            "    #     .. note::\n",
            "    #         `c++` is only available on C++ devices. This function requires `c++` to be installed.\n",
            "    #\n",
            "    #     .. warning::\n",
            "    #         This function requires `\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean(raw_code: str):\n",
        "    cleaned_code = re.sub(r'\\\\n', '\\n', raw_code)\n",
        "    cleaned_code = re.sub(r'\\\\t', '\\t', cleaned_code)\n",
        "    cleaned_code = re.sub(r'\\s+\\n', '\\n', cleaned_code)\n",
        "    return cleaned_code.strip()\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Prompt\": ml_prompt_list,\n",
        "    \"Custom (temperature=0.85)\": [clean(custom_model_output) for custom_model_output in custom_model_outputs],\n",
        "    \"Pretrained (temperature=0.8)\": [clean(pretrained_model_output) for pretrained_model_output in pretrained_model_outputs],\n",
        "    \"Finetuned (temperature=0.8)\": [clean(finetuned_model_output) for finetuned_model_output in finetuned_model_outputs]\n",
        "    })\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    print(f\"Prompt {i+1}: {row['Prompt']}\")\n",
        "    print('-'*100)\n",
        "    print(f\"{'+'*10} Custom (temperature=0.85) {'+'*10}\\n{row['Custom (temperature=0.85)']}\\n\")\n",
        "    print(f\"{'+'*10} Pretrained (temperature=0.8) {'+'*10}\\n{row['Pretrained (temperature=0.8)']}\\n\")\n",
        "    print(f\"{'+'*10} Finetuned (temperature=0.8) {'+'*10}\\n{row['Finetuned (temperature=0.8)']}\\n\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QlaRufDh615"
      },
      "source": [
        "### Summary\n",
        "- Custom transformer model, unsuprisingly, generates random things because of lack of the training data (impossible to train a gen-model built from scratch locally). However, at least it does generate some Python-like syntaxes such as 'self' and 'return,' with appropriate temperature - in this case, 0.85.\n",
        "- Pretrained model without fine-tuning does a descent task. The resulted codes are readable and accurate. However, it sometimes generates a too long code or even worse, gets stuck with commented/uncommented senteces. It also takes this model longer time to predict an output code.\n",
        "- Fine-tuned pretrained model is overall the best model of all the models. It generates mostly function-styled Python code with descent accuracy. However, since the dataset used to fine tune was not expecting the output with source information (copy right), so the most outputs might omit this important part. For the task completion, it does a very good work.\n",
        "- Overall, fine-tuned model is selected since this project requires more accurate, concise, and faster code generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9Pp-iOJliCC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
