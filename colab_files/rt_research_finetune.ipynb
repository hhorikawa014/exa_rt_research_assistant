{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMmYy3zKYLwEbocr6+i8Hur"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"08cf7d2c29f048d39f4afa785d5a8cd0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8164e5ea79d5451e96673144af6e42b7","IPY_MODEL_242d775d5f4943b3bc66bac89d7552c5","IPY_MODEL_16a376b00154404aadf5cb4f9f93db67"],"layout":"IPY_MODEL_2b3932f5a22a49abbf072d11471a20c1"}},"8164e5ea79d5451e96673144af6e42b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_254fc0bbca11400f9c98b5140f1a2408","placeholder":"​","style":"IPY_MODEL_ecff92b355f0478197c32c777da161d7","value":"Map: 100%"}},"242d775d5f4943b3bc66bac89d7552c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d65777012a2348c599e0cedf84bc79dc","max":20000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bbfd55ef406b430b8157e14e16630733","value":20000}},"16a376b00154404aadf5cb4f9f93db67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c9ead2064c94e90a0776f660595a054","placeholder":"​","style":"IPY_MODEL_4a7db0453f744beeb677a1849f8117bf","value":" 20000/20000 [00:42&lt;00:00, 604.22 examples/s]"}},"2b3932f5a22a49abbf072d11471a20c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"254fc0bbca11400f9c98b5140f1a2408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecff92b355f0478197c32c777da161d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d65777012a2348c599e0cedf84bc79dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbfd55ef406b430b8157e14e16630733":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c9ead2064c94e90a0776f660595a054":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a7db0453f744beeb677a1849f8117bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qtp_nyJ-TUQ_","executionInfo":{"status":"ok","timestamp":1742945020855,"user_tz":420,"elapsed":811,"user":{"displayName":"H H","userId":"04764467037930588626"}},"outputId":"5405e012-80f7-423d-8f88-147a15bec9d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/"]},{"cell_type":"code","source":["pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXkmTzd9UlkO","executionInfo":{"status":"ok","timestamp":1742945023176,"user_tz":420,"elapsed":2318,"user":{"displayName":"H H","userId":"04764467037930588626"}},"outputId":"b7cc9b4d-12bd-4d28-f8ed-44b0d0c884ac"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","from datasets import load_dataset"],"metadata":{"id":"muGCNft2TYl-","executionInfo":{"status":"ok","timestamp":1742945023178,"user_tz":420,"elapsed":1,"user":{"displayName":"H H","userId":"04764467037930588626"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def prepare_pretrained_model(model_name=\"Salesforce/codegen-350M-mono\"):\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","    return tokenizer, model\n","\n","\n","def preprocess(example, tokenizer):\n","    # causal masking internally blocks to know the information of output with the algorithm model can see tokens up to the current token\n","    prompt = f\"Generate Python code: {example['func_documentation_string']}\"\n","    code = example[\"func_code_string\"]\n","\n","    # tokenize, with ratio 150:874 for prompt, code respectively\n","\n","    prompt_inputs = tokenizer(prompt, truncation=True, max_length=150, padding=\"max_length\")\n","    code_inputs = tokenizer(code, truncation=True, max_length=874, padding=\"max_length\")\n","\n","    inputs = {\n","        \"input_ids\": prompt_inputs[\"input_ids\"] + code_inputs[\"input_ids\"],\n","        \"attention_mask\": prompt_inputs[\"attention_mask\"] + code_inputs[\"attention_mask\"]\n","    }\n","\n","    # Ensure the total length does not exceed 1024\n","    inputs[\"input_ids\"] = inputs[\"input_ids\"][:1024]\n","    inputs[\"attention_mask\"] = inputs[\"attention_mask\"][:1024]\n","\n","    # Pad to 1024 if necessary\n","    padding_length = 1024 - len(inputs[\"input_ids\"])\n","    inputs[\"input_ids\"] += [tokenizer.pad_token_id] * padding_length\n","    inputs[\"attention_mask\"] += [0] * padding_length\n","\n","    labels = inputs[\"input_ids\"].copy()  # ~1024 tokens\n","\n","    # mask the prompt to only generate the coding part\n","    prompt_len = len(prompt_inputs.input_ids)  # ~150\n","    labels[:prompt_len] = [-100]*prompt_len\n","    inputs[\"labels\"] = labels\n","\n","    return inputs\n","\n","\n","def prepare_dataset(model_name=\"Salesforce/codegen-350M-mono\"):\n","    dataset = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n","    tokenizer, model = prepare_pretrained_model(model_name)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    train = dataset[\"train\"].shuffle(seed=42).select(range(100000)).map(preprocess, fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=dataset[\"train\"].column_names)\n","    valid = dataset[\"validation\"].shuffle(seed=42).select(range(20000)).map(preprocess, fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=dataset[\"validation\"].column_names)\n","\n","    return tokenizer, model, train, valid\n","\n","\n","def fine_tune_pretrained_model(tokenizer, model, train, valid):\n","    # tokenizer, model = prepare_pretrained_model(model_name)\n","    # tokenizer.pad_token = tokenizer.eos_token\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    # train = dataset[\"train\"].map(preprocess, fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=dataset[\"train\"].column_names)\n","    # valid = dataset[\"validation\"].map(preprocess, fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=dataset[\"validation\"].column_names)\n","\n","    training_args = TrainingArguments(\n","        output_dir=\"./codegen-finetuned\",\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        num_train_epochs=2,\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        save_total_limit=1,\n","        learning_rate=5e-5,\n","        weight_decay=0.01,\n","        warmup_steps=100,\n","        fp16=torch.cuda.is_available(),\n","        report_to=\"none\"\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train,\n","        eval_dataset=valid,\n","        tokenizer=tokenizer\n","    )\n","\n","    # train\n","    trainer.train()\n","\n","    # save\n","    trainer.save_model(\"./codegen-finetuned\")\n","    tokenizer.save_pretrained(\"./codegen-finetuned\")\n"],"metadata":{"id":"O5U_GQ6KTjzu","executionInfo":{"status":"ok","timestamp":1742949304325,"user_tz":420,"elapsed":2,"user":{"displayName":"H H","userId":"04764467037930588626"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["tokenizer, model, train, valid = prepare_dataset()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123,"referenced_widgets":["08cf7d2c29f048d39f4afa785d5a8cd0","8164e5ea79d5451e96673144af6e42b7","242d775d5f4943b3bc66bac89d7552c5","16a376b00154404aadf5cb4f9f93db67","2b3932f5a22a49abbf072d11471a20c1","254fc0bbca11400f9c98b5140f1a2408","ecff92b355f0478197c32c777da161d7","d65777012a2348c599e0cedf84bc79dc","bbfd55ef406b430b8157e14e16630733","6c9ead2064c94e90a0776f660595a054","4a7db0453f744beeb677a1849f8117bf"]},"id":"U_GnG1ANUGBp","executionInfo":{"status":"ok","timestamp":1742949350843,"user_tz":420,"elapsed":43780,"user":{"displayName":"H H","userId":"04764467037930588626"}},"outputId":"f243890f-64e8-4aad-d611-8ae23d56eb16"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n","- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08cf7d2c29f048d39f4afa785d5a8cd0"}},"metadata":{}}]},{"cell_type":"code","source":["fine_tune_pretrained_model(tokenizer, model, train, valid)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"id":"OR_p6-w_a9zd","executionInfo":{"status":"ok","timestamp":1742962657691,"user_tz":420,"elapsed":13299991,"user":{"displayName":"H H","userId":"04764467037930588626"}},"outputId":"ce659276-2dc3-4fff-8b3c-c73adb759237"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-25-e36775ad451b>:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='50000' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [50000/50000 3:41:32, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.288700</td>\n","      <td>0.315531</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.196500</td>\n","      <td>0.316068</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"gVOoYL7tpm92"},"execution_count":null,"outputs":[]}]}